{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHVbMAUWrc4V",
        "outputId": "4b13ada9-f256-4eb3-eda9-d2ebe53a2d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-luLdiJs_TM",
        "outputId": "5400eb69-eaa2-4ddd-817b-2f5dcb501870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'VLM-Finetuning-PSL'...\n",
            "remote: Enumerating objects: 1206, done.\u001b[K\n",
            "remote: Counting objects: 100% (223/223), done.\u001b[K\n",
            "remote: Compressing objects: 100% (152/152), done.\u001b[K\n",
            "remote: Total 1206 (delta 155), reused 131 (delta 70), pack-reused 983 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1206/1206), 59.29 MiB | 68.21 MiB/s, done.\n",
            "Resolving deltas: 100% (514/514), done.\n",
            "drive  sample_data  VLM-Finetuning-PSL\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/omerAJ/VLM-Finetuning-PSL\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1f9d280",
        "outputId": "3c574674-a10b-41b0-ec86-acdb8cb893be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'VLM-Finetuning-PSL'\n",
            "/content/VLM-Finetuning-PSL/internvl/internvl_chat\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 7 (delta 6), reused 7 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (7/7), 625 bytes | 312.00 KiB/s, done.\n",
            "From https://github.com/omerAJ/VLM-Finetuning-PSL\n",
            "   57d877a..13e8f2f  main       -> origin/main\n",
            "Updating 57d877a..13e8f2f\n",
            "Fast-forward\n",
            " internvl/internvl_chat_llava/llava/train/train.py | 1 \u001b[32m+\u001b[m\n",
            " 1 file changed, 1 insertion(+)\n"
          ]
        }
      ],
      "source": [
        "%cd VLM-Finetuning-PSL\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OZzcqAUMv6sR",
        "outputId": "e2ef7d12-a93a-461d-db96-4741936171ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/VLM-Finetuning-PSL/internvl\n",
            "Collecting accelerate<1 (from -r requirements/internvl_chat.txt (line 1))\n",
            "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting bitsandbytes (from -r requirements/internvl_chat.txt (line 2))\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting decord (from -r requirements/internvl_chat.txt (line 3))\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Collecting deepspeed>=0.13.5 (from -r requirements/internvl_chat.txt (line 4))\n",
            "  Downloading deepspeed-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops==0.6.1 (from -r requirements/internvl_chat.txt (line 5))\n",
            "  Downloading einops-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting einops-exts==0.0.4 (from -r requirements/internvl_chat.txt (line 6))\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from -r requirements/internvl_chat.txt (line 7)) (0.34.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from -r requirements/internvl_chat.txt (line 8)) (2.37.0)\n",
            "Collecting numpy==1.26.4 (from -r requirements/internvl_chat.txt (line 9))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements/internvl_chat.txt (line 10)) (4.12.0.88)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from -r requirements/internvl_chat.txt (line 11)) (3.11.1)\n",
            "Collecting peft==0.10.0 (from -r requirements/internvl_chat.txt (line 12))\n",
            "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pycocoevalcap (from -r requirements/internvl_chat.txt (line 13))\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r requirements/internvl_chat.txt (line 14)) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements/internvl_chat.txt (line 15)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements/internvl_chat.txt (line 16)) (1.16.0)\n",
            "Collecting sentencepiece==0.1.99 (from -r requirements/internvl_chat.txt (line 17))\n",
            "  Downloading sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting shortuuid (from -r requirements/internvl_chat.txt (line 18))\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting tensorboardX (from -r requirements/internvl_chat.txt (line 19))\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from -r requirements/internvl_chat.txt (line 20)) (3.1.0)\n",
            "Collecting timm==0.9.12 (from -r requirements/internvl_chat.txt (line 21))\n",
            "  Downloading timm-0.9.12-py3-none-any.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.15.1 (from -r requirements/internvl_chat.txt (line 22))\n",
            "  Downloading tokenizers-0.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from -r requirements/internvl_chat.txt (line 23)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.15 in /usr/local/lib/python3.11/dist-packages (from -r requirements/internvl_chat.txt (line 24)) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements/internvl_chat.txt (line 25)) (4.67.1)\n",
            "Collecting transformers==4.37.2 (from -r requirements/internvl_chat.txt (line 26))\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yacs (from -r requirements/internvl_chat.txt (line 27))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (from -r requirements/classification.txt (line 1)) (5.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0->-r requirements/internvl_chat.txt (line 12)) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0->-r requirements/internvl_chat.txt (line 12)) (5.9.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0->-r requirements/internvl_chat.txt (line 12)) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2->-r requirements/internvl_chat.txt (line 26)) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2->-r requirements/internvl_chat.txt (line 26)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2->-r requirements/internvl_chat.txt (line 26)) (2.32.3)\n",
            "Collecting hjson (from deepspeed>=0.13.5->-r requirements/internvl_chat.txt (line 4))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.13.5->-r requirements/internvl_chat.txt (line 4)) (1.1.1)\n",
            "Collecting ninja (from deepspeed>=0.13.5->-r requirements/internvl_chat.txt (line 4))\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.13.5->-r requirements/internvl_chat.txt (line 4)) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.13.5->-r requirements/internvl_chat.txt (line 4)) (2.11.7)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed>=0.13.5->-r requirements/internvl_chat.txt (line 4)) (12.575.51)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->-r requirements/internvl_chat.txt (line 7)) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->-r requirements/internvl_chat.txt (line 7)) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->-r requirements/internvl_chat.txt (line 7)) (1.1.5)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio->-r requirements/internvl_chat.txt (line 8)) (11.3.0)\n",
            "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python (from -r requirements/internvl_chat.txt (line 10))\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap->-r requirements/internvl_chat.txt (line 13)) (2.0.10)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.2->-r requirements/internvl_chat.txt (line 15)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.2->-r requirements/internvl_chat.txt (line 15)) (3.6.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX->-r requirements/internvl_chat.txt (line 19)) (5.29.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->-r requirements/internvl_chat.txt (line 23)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->-r requirements/internvl_chat.txt (line 23)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2->-r requirements/internvl_chat.txt (line 23))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2->-r requirements/internvl_chat.txt (line 23))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2->-r requirements/internvl_chat.txt (line 23))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2->-r requirements/internvl_chat.txt (line 23))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2->-r requirements/internvl_chat.txt (line 23))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2->-r requirements/internvl_chat.txt (line 23))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2->-r requirements/internvl_chat.txt (line 23))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2->-r requirements/internvl_chat.txt (line 23))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2->-r requirements/internvl_chat.txt (line 23))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->-r requirements/internvl_chat.txt (line 23)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->-r requirements/internvl_chat.txt (line 23)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->-r requirements/internvl_chat.txt (line 23)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2->-r requirements/internvl_chat.txt (line 23))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->-r requirements/internvl_chat.txt (line 23)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->-r requirements/internvl_chat.txt (line 23)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->-r requirements/internvl_chat.txt (line 23)) (1.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown->-r requirements/classification.txt (line 1)) (4.13.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed>=0.13.5->-r requirements/internvl_chat.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed>=0.13.5->-r requirements/internvl_chat.txt (line 4)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed>=0.13.5->-r requirements/internvl_chat.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown->-r requirements/classification.txt (line 1)) (2.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->-r requirements/internvl_chat.txt (line 23)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2->-r requirements/internvl_chat.txt (line 26)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2->-r requirements/internvl_chat.txt (line 26)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2->-r requirements/internvl_chat.txt (line 26)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2->-r requirements/internvl_chat.txt (line 26)) (2025.7.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->-r requirements/classification.txt (line 1)) (1.7.1)\n",
            "Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m138.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.17.3-py3-none-any.whl size=1703970 sha256=b08c4f0e6eb86bc8f19426768e5d56182620cf021b06074f1d12aac9de160c46\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/36/9c/0659a1952ed8466bc0ecd23bd59dad485567af5d8c5684c90b\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: sentencepiece, hjson, yacs, shortuuid, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, einops, tensorboardX, opencv-python, nvidia-cusparse-cu12, nvidia-cudnn-cu12, einops-exts, decord, tokenizers, pycocoevalcap, nvidia-cusolver-cu12, transformers, deepspeed, bitsandbytes, accelerate, timm, peft\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.2.0\n",
            "    Uninstalling sentencepiece-0.2.0:\n",
            "      Successfully uninstalled sentencepiece-0.2.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.1\n",
            "    Uninstalling einops-0.8.1:\n",
            "      Successfully uninstalled einops-0.8.1\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.12.0.88\n",
            "    Uninstalling opencv-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-4.12.0.88\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.54.0\n",
            "    Uninstalling transformers-4.54.0:\n",
            "      Successfully uninstalled transformers-4.54.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.9.0\n",
            "    Uninstalling accelerate-1.9.0:\n",
            "      Successfully uninstalled accelerate-1.9.0\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.19\n",
            "    Uninstalling timm-1.0.19:\n",
            "      Successfully uninstalled timm-1.0.19\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.16.0\n",
            "    Uninstalling peft-0.16.0:\n",
            "      Successfully uninstalled peft-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.34.2 bitsandbytes-0.46.1 decord-0.6.0 deepspeed-0.17.3 einops-0.6.1 einops-exts-0.0.4 hjson-3.1.0 ninja-1.11.1.4 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opencv-python-4.11.0.86 peft-0.10.0 pycocoevalcap-1.2 sentencepiece-0.1.99 shortuuid-1.0.13 tensorboardX-2.6.4 timm-0.9.12 tokenizers-0.15.1 transformers-4.37.2 yacs-0.1.8\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "c99102a661134c6f9abad4bf0f27972c",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Installing reqs\n",
        "%cd VLM-Finetuning-PSL/internvl\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr9DMDKKzEOp"
      },
      "source": [
        "-----------\n",
        "pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhiUYyvqOUZu"
      },
      "outputs": [],
      "source": [
        "# !hf download OpenGVLab/InternVL3-1B --local-dir /content/drive/MyDrive/finetuning-output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiQgIhfxzDyr",
        "outputId": "49479805-0a1c-4b95-e69c-036f3abbc22b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/VLM-Finetuning-PSL/internvl\n",
            "/content/VLM-Finetuning-PSL/internvl/pretrained\n",
            "Fetching 20 files:   0% 0/20 [00:00<?, ?it/s]Downloading 'configuration_internvl_chat.py' to 'InternVL3-1B/.cache/huggingface/download/DsLREbI1OaCt0aQRMkT6-8j8O0Q=.502772cc6e24e6d1fbc5bf91ce76c9b234101eaf.incomplete'\n",
            "Downloading 'config.json' to 'InternVL3-1B/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.61eb4a958839440535b68f427427cd69366c1956.incomplete'\n",
            "Downloading 'configuration_intern_vit.py' to 'InternVL3-1B/.cache/huggingface/download/_MrF22XZuffGbJP8rB83lHzm0u4=.7e630c456eb9cf350e55bf850c3ff72f445a7e17.incomplete'\n",
            "Downloading 'conversation.py' to 'InternVL3-1B/.cache/huggingface/download/DEizyvx9GSqAZTOB7ZJABNlK3H0=.5a771766f21ce3aeeb99b286fb8d188b0038a547.incomplete'\n",
            "Downloading 'added_tokens.json' to 'InternVL3-1B/.cache/huggingface/download/SeqzFlf9ZNZ3or_wZAOIdsM3Yxw=.dd972e4080e791eab591742c1168ee7fd6279146.incomplete'\n",
            "Downloading 'examples/image1.jpg' to 'InternVL3-1B/.cache/huggingface/download/examples/DHLHdPXckD7SCnfl3p4-2q-6a4Q=.fd9891ef7e00774157a9dcd726b2ea9fa0c5ecff.incomplete'\n",
            "\n",
            "configuration_internvl_chat.py: 4.04kB [00:00, 23.3MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/configuration_internvl_chat.py\n",
            "\n",
            "config.json: 0.00B [00:00, ?B/s]\u001b[A\n",
            "\n",
            "config.json: 6.33kB [00:00, 14.1MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/config.json\n",
            "configuration_intern_vit.py: 5.55kB [00:00, 12.8MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/configuration_intern_vit.py\n",
            "\n",
            "conversation.py: 15.3kB [00:00, 8.42MB/s]\n",
            "\n",
            "added_tokens.json:   0% 0.00/790 [00:00<?, ?B/s]\u001b[ADownload complete. Moving file to InternVL3-1B/conversation.py\n",
            "added_tokens.json: 100% 790/790 [00:00<00:00, 7.65MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/added_tokens.json\n",
            "\n",
            "image1.jpg:   0% 0.00/78.1k [00:00<?, ?B/s]\u001b[ADownloading '.gitattributes' to 'InternVL3-1B/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.772d8e84809a01c0f4fb9509a89d2f60ca5e8496.incomplete'\n",
            "Downloading 'README.md' to 'InternVL3-1B/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.a10d2b6b5c3459a8c5952731c318e3ffa14566eb.incomplete'\n",
            "image1.jpg: 100% 78.1k/78.1k [00:00<00:00, 3.48MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/examples/image1.jpg\n",
            "Downloading 'examples/image2.jpg' to 'InternVL3-1B/.cache/huggingface/download/examples/3C9-6_0ixe-0KhycCBHC5dl9M-w=.08487494b8dc08d44bc36491adf3ab89ff30d13a3122da86f3cd67cad89eeee8.incomplete'\n",
            "\n",
            ".gitattributes: 1.63kB [00:00, 13.1MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/.gitattributes\n",
            "Fetching 20 files:   5% 1/20 [00:00<00:02,  6.51it/s]\n",
            "README.md: 35.8kB [00:00, 75.6MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/README.md\n",
            "Downloading 'examples/red-panda.mp4' to 'InternVL3-1B/.cache/huggingface/download/examples/b499fO5pocl6nV8kye2RCPcqkBk=.d921c07bb97224d65a37801541d246067f0d506f08723ffa1ad85c217907ccb8.incomplete'\n",
            "Downloading 'generation_config.json' to 'InternVL3-1B/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.467e2ba8edc477ad1fb650f892ab5be9210522ab.incomplete'\n",
            "Downloading 'model.safetensors' to 'InternVL3-1B/.cache/huggingface/download/xGOKKLRSlIhH692hSVvI1-gpoa8=.a8b67c54568417f3631723e6b3e120720eaa638e03e62dc25666c70e3ae3e484.incomplete'\n",
            "Downloading 'merges.txt' to 'InternVL3-1B/.cache/huggingface/download/PtHk0z_I45atnj23IIRhTExwT3w=.31349551d90c7606f325fe0f11bbb8bd5fa0d7c7.incomplete'\n",
            "\n",
            "generation_config.json: 100% 69.0/69.0 [00:00<00:00, 866kB/s]\n",
            "Download complete. Moving file to InternVL3-1B/generation_config.json\n",
            "\n",
            "merges.txt: 0.00B [00:00, ?B/s]\u001b[ADownloading 'modeling_intern_vit.py' to 'InternVL3-1B/.cache/huggingface/download/RTKxUQNMcYzd9NdDH76_hzPG1AI=.eb4aef44ddace5782035f0316de3834563b1c8dd.incomplete'\n",
            "Downloading 'modeling_internvl_chat.py' to 'InternVL3-1B/.cache/huggingface/download/0iPZ4DgR3ByT0g6iDwA8qs1ngbc=.ccf2ab9cc83283f41aafb611d379c4cc50614132.incomplete'\n",
            "\n",
            "\n",
            "model.safetensors:   0% 0.00/1.88G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "modeling_intern_vit.py: 18.1kB [00:00, 70.7MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/modeling_intern_vit.py\n",
            "Downloading 'preprocessor_config.json' to 'InternVL3-1B/.cache/huggingface/download/PYH5dHjks7Ei0Yd3X0Z8xIwsCNQ=.dfd7e50d9d4e67cd679b16b337b419a0c6cfa849.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "modeling_internvl_chat.py: 16.0kB [00:00, 60.4MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/modeling_internvl_chat.py\n",
            "\n",
            "\n",
            "\n",
            "image2.jpg:   0% 0.00/126k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "preprocessor_config.json: 100% 287/287 [00:00<00:00, 2.84MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/preprocessor_config.json\n",
            "Downloading 'special_tokens_map.json' to 'InternVL3-1B/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.ac23c0aaa2434523c494330aeb79c58395378103.incomplete'\n",
            "merges.txt: 1.67MB [00:00, 26.7MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/merges.txt\n",
            "\n",
            "special_tokens_map.json: 100% 613/613 [00:00<00:00, 6.75MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/special_tokens_map.json\n",
            "\n",
            "image2.jpg: 100% 126k/126k [00:00<00:00, 4.56MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/examples/image2.jpg\n",
            "Fetching 20 files:  45% 9/20 [00:00<00:00, 39.06it/s]Downloading 'tokenizer_config.json' to 'InternVL3-1B/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.e9b23c6972b2bb09d1a930cdced6f164031f2392.incomplete'\n",
            "Downloading 'tokenizer.json' to 'InternVL3-1B/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.1b4f039248a420730cd195d6d6a1a9cc713a7f14.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 8.92kB [00:00, 41.0MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/tokenizer_config.json\n",
            "Downloading 'vocab.json' to 'InternVL3-1B/.cache/huggingface/download/j3m-Hy6QvBddw8RXA1uSWl1AJ0c=.6bce3a0a3866c4791a74d83d78f6824c3af64ec3.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "red-panda.mp4: 100% 1.87M/1.87M [00:00<00:00, 26.7MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/examples/red-panda.mp4\n",
            "\n",
            "\n",
            "tokenizer.json: 7.03MB [00:00, 91.0MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/tokenizer.json\n",
            "vocab.json: 3.38MB [00:00, 47.3MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/vocab.json\n",
            "\n",
            "\n",
            "model.safetensors:   2% 41.9M/1.88G [00:00<00:10, 170MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   4% 73.4M/1.88G [00:00<00:08, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   6% 105M/1.88G [00:00<00:07, 222MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   7% 136M/1.88G [00:00<00:07, 236MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   9% 168M/1.88G [00:00<00:06, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  11% 199M/1.88G [00:00<00:06, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  12% 231M/1.88G [00:01<00:06, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  14% 262M/1.88G [00:01<00:06, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  16% 294M/1.88G [00:01<00:06, 252MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  17% 325M/1.88G [00:01<00:06, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  19% 357M/1.88G [00:01<00:06, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  21% 388M/1.88G [00:01<00:05, 252MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  22% 419M/1.88G [00:01<00:05, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  24% 451M/1.88G [00:01<00:05, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  26% 482M/1.88G [00:02<00:05, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  27% 514M/1.88G [00:02<00:05, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  29% 545M/1.88G [00:02<00:05, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  31% 577M/1.88G [00:02<00:05, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  32% 608M/1.88G [00:02<00:04, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  34% 640M/1.88G [00:02<00:04, 260MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  36% 671M/1.88G [00:02<00:04, 264MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  37% 703M/1.88G [00:02<00:04, 267MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  39% 734M/1.88G [00:02<00:04, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  41% 765M/1.88G [00:03<00:04, 261MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  42% 797M/1.88G [00:03<00:04, 260MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  44% 828M/1.88G [00:03<00:04, 260MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  46% 860M/1.88G [00:03<00:04, 252MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  47% 891M/1.88G [00:03<00:03, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  49% 923M/1.88G [00:03<00:03, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  51% 954M/1.88G [00:03<00:03, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  53% 986M/1.88G [00:03<00:03, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  54% 1.02G/1.88G [00:04<00:03, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  56% 1.05G/1.88G [00:04<00:03, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  58% 1.08G/1.88G [00:04<00:03, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  59% 1.11G/1.88G [00:04<00:03, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  61% 1.14G/1.88G [00:04<00:03, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  63% 1.17G/1.88G [00:04<00:02, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  64% 1.21G/1.88G [00:04<00:02, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  66% 1.24G/1.88G [00:05<00:02, 236MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  68% 1.27G/1.88G [00:05<00:02, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  69% 1.30G/1.88G [00:05<00:02, 226MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  71% 1.33G/1.88G [00:05<00:02, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  73% 1.36G/1.88G [00:05<00:02, 232MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  74% 1.39G/1.88G [00:05<00:02, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  76% 1.43G/1.88G [00:05<00:01, 241MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  78% 1.46G/1.88G [00:05<00:01, 241MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  79% 1.49G/1.88G [00:06<00:01, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  81% 1.52G/1.88G [00:06<00:01, 236MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  83% 1.55G/1.88G [00:06<00:01, 236MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  84% 1.58G/1.88G [00:06<00:01, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  86% 1.61G/1.88G [00:06<00:01, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  88% 1.65G/1.88G [00:06<00:01, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  89% 1.68G/1.88G [00:06<00:00, 223MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  91% 1.71G/1.88G [00:07<00:00, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  93% 1.74G/1.88G [00:07<00:00, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  94% 1.77G/1.88G [00:07<00:00, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  96% 1.80G/1.88G [00:07<00:00, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  98% 1.84G/1.88G [00:07<00:00, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors: 100% 1.88G/1.88G [00:07<00:00, 244MB/s]\n",
            "Download complete. Moving file to InternVL3-1B/model.safetensors\n",
            "Fetching 20 files: 100% 20/20 [00:07<00:00,  2.52it/s]\n",
            "/content/VLM-Finetuning-PSL/internvl/pretrained/InternVL3-1B\n"
          ]
        }
      ],
      "source": [
        "%cd /content/VLM-Finetuning-PSL/internvl\n",
        "!mkdir -p pretrained\n",
        "%cd pretrained\n",
        "!hf download OpenGVLab/InternVL3-1B --local-dir InternVL3-1B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwV9dzvo5bLa"
      },
      "source": [
        "-------------------\n",
        "Structuring dataset into jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SCwlz3y5a5y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "video_dir = \"/content/drive/MyDrive/PSL_Data_segmentation/Words\"\n",
        "jsonl_path = \"/content/VLM-Finetuning-PSL/internvl/internvl_chat/shell/data/dataset.jsonl\"\n",
        "\n",
        "with open(jsonl_path, \"w\") as out_file:\n",
        "    for idx, filename in enumerate(os.listdir(video_dir)):\n",
        "        if filename.endswith(\".mp4\"):\n",
        "            word = os.path.splitext(filename)[0]  # title is the label\n",
        "            entry = {\n",
        "                \"type\": \"video\",\n",
        "                \"url\": filename,\n",
        "                \"conversations\": [\n",
        "                    {\"from\": \"human\", \"value\": \"<video>\\nWhich word does the person sign in this video?\"},\n",
        "                    {\"from\": \"gpt\", \"value\": word}\n",
        "                ]\n",
        "            }\n",
        "            out_file.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh9zN3Z40cPc",
        "outputId": "503751bd-11b2-47a3-f8d3-eb22ca10a628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/VLM-Finetuning-PSL\n"
          ]
        }
      ],
      "source": [
        "# create data json\n",
        "import json\n",
        "%cd /content/VLM-Finetuning-PSL\n",
        "data = {\n",
        "    \"train\": {\n",
        "        \"root\": \"/content/drive/MyDrive/PSL_Data_segmentation/Words\",\n",
        "        \"annotation\": \"/content/VLM-Finetuning-PSL/internvl/internvl_chat/shell/data/dataset.jsonl\",\n",
        "        \"data_augment\": False,\n",
        "        \"max_dynamic_patch\": 12,\n",
        "        \"repeat_time\": 1,\n",
        "        \"length\": 51\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"config.json\", \"w\") as f:\n",
        "    json.dump(data, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "QKfoBmgvVaWQ",
        "outputId": "9eba3e8b-6f62-4cb7-fcbb-ef6350b35852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.37.2\n",
            "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.37.2)\n",
            "  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (2025.7.14)\n",
            "Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.4\n",
            "    Uninstalling tokenizers-0.21.4:\n",
            "      Successfully uninstalled tokenizers-0.21.4\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.54.1\n",
            "    Uninstalling transformers-4.54.1:\n",
            "      Successfully uninstalled transformers-4.54.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.37.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "949619fbfb674161bb795db13f087179",
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install transformers==4.37.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KgWnlK4HHac",
        "outputId": "9ef88ab1-110f-4af7-d03f-02b2f459e2d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/VLM-Finetuning-PSL/internvl/internvl_chat\n",
            "+ GPUS=1\n",
            "+ BATCH_SIZE=128\n",
            "+ PER_DEVICE_BATCH_SIZE=32\n",
            "+ GRADIENT_ACC=4\n",
            "+ pwd\n",
            "+ export PYTHONPATH=/env/python:/content/VLM-Finetuning-PSL/internvl/internvl_chat\n",
            "+ export MASTER_PORT=34229\n",
            "+ export TF_CPP_MIN_LOG_LEVEL=3\n",
            "+ export LAUNCHER=pytorch\n",
            "+ OUTPUT_DIR=/content/drive/MyDrive/finetuning-output-Take1\n",
            "+ [ ! -d /content/drive/MyDrive/finetuning-output-Take1 ]\n",
            "+ torchrun --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --nproc_per_node=1 --master_port=34229 internvl/train/internvl_chat_finetune.py --model_name_or_path OpenGVLab/InternVL3-1B --conv_style internvl2_5 --use_fast_tokenizer False --output_dir /content/drive/MyDrive/finetuning-output-Take1 --meta_path /content/VLM-Finetuning-PSL/config.json --overwrite_output_dir True --force_image_size 448 --max_dynamic_patch 12 --down_sample_ratio 0.5 --drop_path_rate 0.0 --freeze_llm False --freeze_mlp False --freeze_backbone False --vision_select_layer -1 --dataloader_num_workers 4 --bf16 True --num_train_epochs+  1 --per_device_train_batch_size 32 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategytee steps -a --save_steps /content/drive/MyDrive/finetuning-output-Take1/training_log.txt 2000\n",
            " --save_total_limit 1 --learning_rate 1e-5 --weight_decay 0.05 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --max_seq_length 16384 --do_train True --grad_checkpoint True --group_by_length True --dynamic_image_size True --use_thumbnail True --ps_version v2 --deepspeed zero_stage1_config.json --report_to tensorboard --use_backbone_lora 32 --use_llm_lora 32 --min_num_frame 16 --max_num_frame 64\n",
            "[2025-07-31 17:44:08,991] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-07-31 17:44:12,435] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
            "FlashAttention2 is not installed.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753983854.668829   34359 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753983854.675375   34359 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "petrel_client is not installed. If you read data locally instead of from ceph, ignore it.\n",
            "petrel_client is not installed. Using PIL to load images.\n",
            "[2025-07-31 17:44:16,948] [INFO] [comm.py:821:init_distributed] cdb=None\n",
            "[2025-07-31 17:44:16,948] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "07/31/2025 17:44:16 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "07/31/2025 17:44:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=4,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=zero_stage1_config.json,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=True,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/finetuning-output-Take1/runs/Jul31_17-44-16_a7061e0bd157,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=1.0,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.COSINE,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "output_dir=/content/drive/MyDrive/finetuning-output-Take1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/finetuning-output-Take1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=2000,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.03,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.05,\n",
            ")\n",
            "07/31/2025 17:44:16 - INFO - __main__ - Loading Tokenizer: OpenGVLab/InternVL3-1B\n",
            "[INFO|tokenization_utils_base.py:2027] 2025-07-31 17:44:17,122 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--OpenGVLab--InternVL3-1B/snapshots/06cddba9140fdb73a47951480b6e9cec04970559/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2027] 2025-07-31 17:44:17,122 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--OpenGVLab--InternVL3-1B/snapshots/06cddba9140fdb73a47951480b6e9cec04970559/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2027] 2025-07-31 17:44:17,122 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--OpenGVLab--InternVL3-1B/snapshots/06cddba9140fdb73a47951480b6e9cec04970559/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2027] 2025-07-31 17:44:17,122 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--OpenGVLab--InternVL3-1B/snapshots/06cddba9140fdb73a47951480b6e9cec04970559/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2027] 2025-07-31 17:44:17,122 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--OpenGVLab--InternVL3-1B/snapshots/06cddba9140fdb73a47951480b6e9cec04970559/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2027] 2025-07-31 17:44:17,122 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--OpenGVLab--InternVL3-1B/snapshots/06cddba9140fdb73a47951480b6e9cec04970559/tokenizer.json\n",
            "[WARNING|logging.py:314] 2025-07-31 17:44:17,374 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/31/2025 17:44:17 - INFO - __main__ - Loading InternVLChatModel...\n",
            "[INFO|configuration_utils.py:729] 2025-07-31 17:44:17,451 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--OpenGVLab--InternVL3-1B/snapshots/06cddba9140fdb73a47951480b6e9cec04970559/config.json\n",
            "[INFO|configuration_utils.py:792] 2025-07-31 17:44:17,452 >> Model config InternVLChatConfig {\n",
            "  \"_commit_hash\": \"06cddba9140fdb73a47951480b6e9cec04970559\",\n",
            "  \"_name_or_path\": \"/mnt/petrelfs/wangweiyun/workspace_wwy/open_source/InternVL/internvl_chat/work_dirs/internvl_chat_v3_0/InternVL3_0-1B-MPO-try0-2\",\n",
            "  \"architectures\": [\n",
            "    \"InternVLChatModel\"\n",
            "  ],\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"OpenGVLab/InternVL3-1B--configuration_internvl_chat.InternVLChatConfig\",\n",
            "    \"AutoModel\": \"OpenGVLab/InternVL3-1B--modeling_internvl_chat.InternVLChatModel\",\n",
            "    \"AutoModelForCausalLM\": \"OpenGVLab/InternVL3-1B--modeling_internvl_chat.InternVLChatModel\"\n",
            "  },\n",
            "  \"downsample_ratio\": 0.5,\n",
            "  \"dynamic_image_size\": true,\n",
            "  \"force_image_size\": 448,\n",
            "  \"hidden_size\": 896,\n",
            "  \"image_fold\": null,\n",
            "  \"llm_config\": {\n",
            "    \"_attn_implementation_autoset\": true,\n",
            "    \"_name_or_path\": \"./pretrained/Qwen2.5-32B-Instruct\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"Qwen2ForCausalLM\"\n",
            "    ],\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"bad_words_ids\": null,\n",
            "    \"begin_suppress_tokens\": null,\n",
            "    \"bos_token_id\": 151643,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": 151643,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"silu\",\n",
            "    \"hidden_size\": 896,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 4864,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"max_position_embeddings\": 32768,\n",
            "    \"max_window_layers\": 70,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"qwen2\",\n",
            "    \"moe_config\": null,\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"num_attention_heads\": 14,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_hidden_layers\": 24,\n",
            "    \"num_key_value_heads\": 2,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": null,\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"rms_norm_eps\": 1e-06,\n",
            "    \"rope_scaling\": {\n",
            "      \"factor\": 2.0,\n",
            "      \"rope_type\": \"dynamic\",\n",
            "      \"type\": \"dynamic\"\n",
            "    },\n",
            "    \"rope_theta\": 1000000.0,\n",
            "    \"sep_token_id\": null,\n",
            "    \"sliding_window\": null,\n",
            "    \"suppress_tokens\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tf_legacy_loss\": false,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": false,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": \"bfloat16\",\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.37.2\",\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": true,\n",
            "    \"use_cache\": false,\n",
            "    \"use_sliding_window\": false,\n",
            "    \"vocab_size\": 151674\n",
            "  },\n",
            "  \"max_dynamic_patch\": 12,\n",
            "  \"min_dynamic_patch\": 1,\n",
            "  \"model_type\": \"internvl_chat\",\n",
            "  \"pad2square\": false,\n",
            "  \"ps_version\": \"v2\",\n",
            "  \"select_layer\": -1,\n",
            "  \"system_message\": null,\n",
            "  \"template\": \"internvl2_5\",\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": null,\n",
            "  \"use_backbone_lora\": 0,\n",
            "  \"use_llm_lora\": 0,\n",
            "  \"use_thumbnail\": true,\n",
            "  \"vision_config\": {\n",
            "    \"_attn_implementation_autoset\": true,\n",
            "    \"_name_or_path\": \"OpenGVLab/InternViT-6B-448px-V1-5\",\n",
            "    \"add_cross_attention\": false,\n",
            "    \"architectures\": [\n",
            "      \"InternVisionModel\"\n",
            "    ],\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"auto_map\": {\n",
            "      \"AutoConfig\": \"configuration_intern_vit.InternVisionConfig\",\n",
            "      \"AutoModel\": \"modeling_intern_vit.InternVisionModel\"\n",
            "    },\n",
            "    \"bad_words_ids\": null,\n",
            "    \"begin_suppress_tokens\": null,\n",
            "    \"bos_token_id\": null,\n",
            "    \"capacity_factor\": 1.2,\n",
            "    \"chunk_size_feed_forward\": 0,\n",
            "    \"cross_attention_hidden_size\": null,\n",
            "    \"decoder_start_token_id\": null,\n",
            "    \"diversity_penalty\": 0.0,\n",
            "    \"do_sample\": false,\n",
            "    \"drop_path_rate\": 0.1,\n",
            "    \"dropout\": 0.0,\n",
            "    \"early_stopping\": false,\n",
            "    \"encoder_no_repeat_ngram_size\": 0,\n",
            "    \"eos_token_id\": null,\n",
            "    \"eval_capacity_factor\": 1.4,\n",
            "    \"exponential_decay_length_penalty\": null,\n",
            "    \"finetuning_task\": null,\n",
            "    \"forced_bos_token_id\": null,\n",
            "    \"forced_eos_token_id\": null,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_size\": 1024,\n",
            "    \"id2label\": {\n",
            "      \"0\": \"LABEL_0\",\n",
            "      \"1\": \"LABEL_1\"\n",
            "    },\n",
            "    \"image_size\": 448,\n",
            "    \"initializer_factor\": 0.1,\n",
            "    \"initializer_range\": 1e-10,\n",
            "    \"intermediate_size\": 4096,\n",
            "    \"is_decoder\": false,\n",
            "    \"is_encoder_decoder\": false,\n",
            "    \"label2id\": {\n",
            "      \"LABEL_0\": 0,\n",
            "      \"LABEL_1\": 1\n",
            "    },\n",
            "    \"laux_allreduce\": \"all_nodes\",\n",
            "    \"layer_norm_eps\": 1e-06,\n",
            "    \"length_penalty\": 1.0,\n",
            "    \"max_length\": 20,\n",
            "    \"min_length\": 0,\n",
            "    \"model_type\": \"intern_vit_6b\",\n",
            "    \"moe_coeff_ratio\": 0.5,\n",
            "    \"moe_intermediate_size\": 768,\n",
            "    \"moe_output_scale\": 4.0,\n",
            "    \"no_repeat_ngram_size\": 0,\n",
            "    \"noisy_gate_policy\": \"RSample_before\",\n",
            "    \"norm_type\": \"layer_norm\",\n",
            "    \"num_attention_heads\": 16,\n",
            "    \"num_beam_groups\": 1,\n",
            "    \"num_beams\": 1,\n",
            "    \"num_channels\": 3,\n",
            "    \"num_experts\": 8,\n",
            "    \"num_hidden_layers\": 24,\n",
            "    \"num_return_sequences\": 1,\n",
            "    \"num_routed_experts\": 4,\n",
            "    \"num_shared_experts\": 4,\n",
            "    \"output_attentions\": false,\n",
            "    \"output_hidden_states\": false,\n",
            "    \"output_scores\": false,\n",
            "    \"pad_token_id\": null,\n",
            "    \"patch_size\": 14,\n",
            "    \"prefix\": null,\n",
            "    \"problem_type\": null,\n",
            "    \"pruned_heads\": {},\n",
            "    \"qk_normalization\": false,\n",
            "    \"qkv_bias\": true,\n",
            "    \"remove_invalid_values\": false,\n",
            "    \"repetition_penalty\": 1.0,\n",
            "    \"return_dict\": true,\n",
            "    \"return_dict_in_generate\": false,\n",
            "    \"sep_token_id\": null,\n",
            "    \"shared_expert_intermediate_size\": 3072,\n",
            "    \"suppress_tokens\": null,\n",
            "    \"task_specific_params\": null,\n",
            "    \"temperature\": 1.0,\n",
            "    \"tf_legacy_loss\": false,\n",
            "    \"tie_encoder_decoder\": false,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"tokenizer_class\": null,\n",
            "    \"top_k\": 50,\n",
            "    \"top_p\": 1.0,\n",
            "    \"torch_dtype\": \"bfloat16\",\n",
            "    \"torchscript\": false,\n",
            "    \"transformers_version\": \"4.37.2\",\n",
            "    \"typical_p\": 1.0,\n",
            "    \"use_bfloat16\": true,\n",
            "    \"use_flash_attn\": true,\n",
            "    \"use_moe\": false,\n",
            "    \"use_residual\": true,\n",
            "    \"use_rts\": false,\n",
            "    \"use_weighted_residual\": false\n",
            "  }\n",
            "}\n",
            "\n",
            "07/31/2025 17:44:17 - INFO - __main__ - Using flash_attention_2 for LLaMA\n",
            "[INFO|modeling_utils.py:3476] 2025-07-31 17:44:17,454 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--OpenGVLab--InternVL3-1B/snapshots/06cddba9140fdb73a47951480b6e9cec04970559/model.safetensors\n",
            "[INFO|modeling_utils.py:1426] 2025-07-31 17:44:17,476 >> Instantiating InternVLChatModel model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:826] 2025-07-31 17:44:17,477 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|configuration_utils.py:826] 2025-07-31 17:44:17,534 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4350] 2025-07-31 17:44:18,510 >> All model checkpoint weights were used when initializing InternVLChatModel.\n",
            "\n",
            "[INFO|modeling_utils.py:4358] 2025-07-31 17:44:18,510 >> All the weights of InternVLChatModel were initialized from the model checkpoint at OpenGVLab/InternVL3-1B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use InternVLChatModel for predictions without further training.\n",
            "[INFO|configuration_utils.py:781] 2025-07-31 17:44:18,593 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--OpenGVLab--InternVL3-1B/snapshots/06cddba9140fdb73a47951480b6e9cec04970559/generation_config.json\n",
            "[INFO|configuration_utils.py:826] 2025-07-31 17:44:18,594 >> Generate config GenerationConfig {}\n",
            "\n",
            "07/31/2025 17:44:18 - INFO - __main__ - Finished\n",
            "07/31/2025 17:44:18 - INFO - __main__ - model.config.force_image_size: 448\n",
            "07/31/2025 17:44:18 - INFO - __main__ - data_args.force_image_size: 448\n",
            "07/31/2025 17:44:18 - INFO - __main__ - model.config.vision_config.image_size: 448\n",
            "07/31/2025 17:44:18 - INFO - __main__ - max_dynamic_patch is set to 12 according to the meta file\n",
            "07/31/2025 17:44:18 - INFO - __main__ - [Dataset] num_image_token: 256\n",
            "07/31/2025 17:44:18 - INFO - __main__ - [Dataset] dynamic_image_size: True\n",
            "07/31/2025 17:44:18 - INFO - __main__ - [Dataset] use_thumbnail: True\n",
            "07/31/2025 17:44:18 - INFO - __main__ - [Dataset] min_dynamic_patch: 1, max_dynamic_patch: 12\n",
            "07/31/2025 17:44:18 - INFO - __main__ - Formatting inputs...Skip in lazy mode\n",
            "07/31/2025 17:44:18 - INFO - __main__ - Add dataset: train with length: 51\n",
            "trainable params: 12,582,912 || all params: 316,595,200 || trainable%: 3.974448128082801\n",
            "trainable params: 17,596,416 || all params: 647,294,336 || trainable%: 2.7184566620400648\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.0.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.0.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.0.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.0.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.0.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.0.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.0.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.0.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.1.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.1.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.1.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.1.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.1.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.1.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.1.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.1.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.2.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.2.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.2.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.2.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.2.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.2.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.2.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.2.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.3.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.3.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.3.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.3.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.3.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.3.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.3.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.3.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.4.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.4.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.4.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.4.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.4.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.4.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.4.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.4.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.5.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.5.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.5.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.5.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.5.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.5.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.5.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.5.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.6.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.6.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.6.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.6.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.6.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.6.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.6.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.6.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.7.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.7.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.7.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.7.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.7.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.7.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.7.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.7.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.8.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.8.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.8.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.8.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.8.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.8.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.8.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.8.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.9.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.9.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.9.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.9.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.9.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.9.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.9.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.9.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.10.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.10.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.10.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.10.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.10.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.10.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.10.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.10.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.11.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.11.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.11.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.11.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.11.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.11.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.11.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.11.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.12.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.12.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.12.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.12.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.12.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.12.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.12.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.12.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.13.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.13.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.13.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.13.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.13.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.13.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.13.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.13.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.14.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.14.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.14.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.14.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.14.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.14.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.14.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.14.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.15.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.15.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.15.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.15.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.15.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.15.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.15.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.15.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.16.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.16.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.16.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.16.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.16.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.16.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.16.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.16.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.17.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.17.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.17.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.17.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.17.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.17.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.17.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.17.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.18.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.18.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.18.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.18.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.18.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.18.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.18.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.18.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.19.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.19.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.19.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.19.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.19.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.19.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.19.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.19.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.20.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.20.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.20.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.20.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.20.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.20.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.20.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.20.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.21.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.21.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.21.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.21.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.21.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.21.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.21.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.21.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.22.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.22.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.22.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.22.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.22.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.22.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.22.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.22.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.23.attn.qkv.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.23.attn.qkv.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.23.attn.proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.23.attn.proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.23.mlp.fc1.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.23.mlp.fc1.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.23.mlp.fc2.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - vision_model.base_model.model.encoder.layers.23.mlp.fc2.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - language_model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - mlp1.0.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - mlp1.0.bias\n",
            "07/31/2025 17:44:19 - INFO - __main__ - mlp1.1.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - mlp1.1.bias\n",
            "07/31/2025 17:44:19 - INFO - __main__ - mlp1.3.weight\n",
            "07/31/2025 17:44:19 - INFO - __main__ - mlp1.3.bias\n",
            "[INFO|trainer.py:571] 2025-07-31 17:44:19,240 >> Using auto half precision backend\n",
            "[2025-07-31 17:44:19,630] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.3, git-hash=unknown, git-branch=unknown\n",
            "[2025-07-31 17:44:19,630] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 1\n",
            "[2025-07-31 17:44:20,353] [INFO] [engine.py:1339:_configure_distributed_model] ********** distributed groups summary **********\n",
            "\t self.dp_world_size=1\n",
            "\t self.mp_world_size=1\n",
            "\t self.seq_dp_world_size=1\n",
            "\t self.sequence_parallel_size=1\n",
            "***********************************************\n",
            "[2025-07-31 17:44:20,615] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 0.029500484466552734 seconds\n",
            "[2025-07-31 17:44:20,647] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2025-07-31 17:44:20,647] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "[2025-07-31 17:44:20,708] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
            "[2025-07-31 17:44:20,708] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
            "[2025-07-31 17:44:20,708] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer\n",
            "[2025-07-31 17:44:20,708] [INFO] [stage_1_and_2.py:172:__init__] Reduce bucket size 1000000000\n",
            "[2025-07-31 17:44:20,708] [INFO] [stage_1_and_2.py:173:__init__] Allgather bucket size 1000000000\n",
            "[2025-07-31 17:44:20,708] [INFO] [stage_1_and_2.py:174:__init__] CPU Offload: False\n",
            "[2025-07-31 17:44:20,708] [INFO] [stage_1_and_2.py:175:__init__] Round robin gradient partitioning: False\n",
            "[2025-07-31 17:44:21,171] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
            "[2025-07-31 17:44:21,172] [INFO] [utils.py:782:see_memory_usage] MA 2.13 GB         Max_MA 2.2 GB         CA 2.34 GB         Max_CA 2 GB \n",
            "[2025-07-31 17:44:21,172] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.42 GB, percent = 6.5%\n",
            "[2025-07-31 17:44:21,531] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
            "[2025-07-31 17:44:21,531] [INFO] [utils.py:782:see_memory_usage] MA 2.13 GB         Max_MA 2.26 GB         CA 2.47 GB         Max_CA 2 GB \n",
            "[2025-07-31 17:44:21,532] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.42 GB, percent = 6.5%\n",
            "[2025-07-31 17:44:21,532] [INFO] [stage_1_and_2.py:599:__init__] optimizer state initialized\n",
            "[2025-07-31 17:44:21,893] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2025-07-31 17:44:21,893] [INFO] [utils.py:782:see_memory_usage] MA 2.13 GB         Max_MA 2.13 GB         CA 2.47 GB         Max_CA 2 GB \n",
            "[2025-07-31 17:44:21,894] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.42 GB, percent = 6.5%\n",
            "[2025-07-31 17:44:21,904] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
            "[2025-07-31 17:44:21,905] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client callable to create LR scheduler\n",
            "[2025-07-31 17:44:21,905] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x790abe74a710>\n",
            "[2025-07-31 17:44:21,905] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[[0.9, 0.999]]\n",
            "[2025-07-31 17:44:21,913] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True\n",
            "[2025-07-31 17:44:21,913] [INFO] [config.py:954:print] DeepSpeedEngine configuration:\n",
            "[2025-07-31 17:44:21,913] [INFO] [config.py:958:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2025-07-31 17:44:21,913] [INFO] [config.py:958:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
            "[2025-07-31 17:44:21,913] [INFO] [config.py:958:print]   amp_enabled .................. False\n",
            "[2025-07-31 17:44:21,913] [INFO] [config.py:958:print]   amp_params ................... False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   bfloat16_config .............. enabled=True immediate_grad_update=False check_grad_overflow=False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   checkpoint_tag_validation_enabled  True\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   checkpoint_tag_validation_fail  False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x790abe8d05d0>\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   communication_data_type ...... None\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False keep_int_input_tensors=True keep_all_input_tensors=False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   curriculum_enabled_legacy .... False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   curriculum_params_legacy ..... False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   data_efficiency_enabled ...... False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   dataloader_drop_last ......... False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   disable_allgather ............ False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   dump_state ................... False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   eigenvalue_enabled ........... False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   eigenvalue_layer_num ......... 0\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   eigenvalue_max_iter .......... 100\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   eigenvalue_stability ......... 1e-06\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   eigenvalue_tol ............... 0.01\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   eigenvalue_verbose ........... False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   elasticity_enabled ........... False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   float16_config ............... enabled=False auto_cast=True loss_scale=0.0 initial_scale_power=32 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2025-07-31 17:44:21,914] [INFO] [config.py:958:print]   global_rank .................. 0\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   grad_accum_dtype ............. None\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   gradient_accumulation_steps .. 4\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   gradient_clipping ............ 1.0\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   gradient_predivide_factor .... 1.0\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   graph_harvesting ............. False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   load_universal_checkpoint .... False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   memory_breakdown ............. False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   mics_hierarchial_params_gather  False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   mics_shard_size .............. -1\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   optimizer_legacy_fusion ...... False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   optimizer_name ............... adamw\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.05}\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   pld_enabled .................. False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   pld_params ................... False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   prescale_gradients ........... False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   scheduler_name ............... None\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   scheduler_params ............. None\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   sparse_attention ............. None\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   sparse_gradients_enabled ..... False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   steps_per_print .............. inf\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   timers_config ................ enabled=True synchronized=True\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   torch_autocast_dtype ......... None\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   torch_autocast_enabled ....... False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   torch_autocast_lower_precision_safe_modules  None\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   train_batch_size ............. 128\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   train_micro_batch_size_per_gpu  32\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   use_data_before_expert_parallel_  False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   use_node_local_storage ....... False\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   wall_clock_breakdown ......... True\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   weight_quantization_config ... None\n",
            "[2025-07-31 17:44:21,915] [INFO] [config.py:958:print]   world_size ................... 1\n",
            "[2025-07-31 17:44:21,916] [INFO] [config.py:958:print]   zero_allow_untested_optimizer  False\n",
            "[2025-07-31 17:44:21,916] [INFO] [config.py:958:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=1000000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=1000000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
            "[2025-07-31 17:44:21,916] [INFO] [config.py:958:print]   zero_enabled ................. True\n",
            "[2025-07-31 17:44:21,916] [INFO] [config.py:958:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2025-07-31 17:44:21,916] [INFO] [config.py:958:print]   zero_optimization_stage ...... 1\n",
            "[2025-07-31 17:44:21,916] [INFO] [config.py:944:print_user_config]   json = {\n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 1, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 1.000000e+09, \n",
            "        \"overlap_comm\": true, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 1.000000e+09, \n",
            "        \"contiguous_gradients\": true\n",
            "    }, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": false, \n",
            "        \"auto_cast\": true, \n",
            "        \"loss_scale\": 0, \n",
            "        \"initial_scale_power\": 32, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"bf16\": {\n",
            "        \"enabled\": true\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"lr\": 1e-05, \n",
            "            \"betas\": [0.9, 0.999], \n",
            "            \"eps\": 1e-08, \n",
            "            \"weight_decay\": 0.05\n",
            "        }\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 4, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"steps_per_print\": inf, \n",
            "    \"train_batch_size\": 128, \n",
            "    \"train_micro_batch_size_per_gpu\": 32, \n",
            "    \"wall_clock_breakdown\": true\n",
            "}\n",
            "[INFO|trainer.py:1721] 2025-07-31 17:44:21,916 >> ***** Running training *****\n",
            "[INFO|trainer.py:1722] 2025-07-31 17:44:21,916 >>   Num examples = 51\n",
            "[INFO|trainer.py:1723] 2025-07-31 17:44:21,916 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1724] 2025-07-31 17:44:21,916 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1727] 2025-07-31 17:44:21,916 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "[INFO|trainer.py:1728] 2025-07-31 17:44:21,916 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:1729] 2025-07-31 17:44:21,916 >>   Total optimization steps = 1\n",
            "[INFO|trainer.py:1730] 2025-07-31 17:44:21,922 >>   Number of trainable parameters = 34,662,144\n",
            "  0%|          | 0/1 [00:00<?, ?it/s][2025-07-31 17:44:24,462] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-07-31 17:44:27,797] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753983870.084826   34552 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753983870.091237   34552 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2025-07-31 17:44:35,015] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-07-31 17:44:38,334] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753983880.625280   34705 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753983880.631853   34705 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2025-07-31 17:44:45,582] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-07-31 17:44:48,965] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753983891.306043   34858 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753983891.312578   34858 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2025-07-31 17:44:56,258] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-07-31 17:44:59,593] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753983901.874659   35011 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753983901.881115   35011 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "dynamic ViT batch size: 32, images per sample: 1.0, dynamic token length: 62\n",
            "[2025-07-31 17:45:07,269] [INFO] [logging.py:107:log_dist] [Rank 0] time (ms) | fwd_microstep: 1065.62 | bwd_microstep: 1389.43 | bwd_inner_microstep: 1389.14 | bwd_allreduce_microstep: 0.01 | step_microstep: 0.01\n",
            "FlashAttention2 is not installed.\n",
            "petrel_client is not installed. If you read data locally instead of from ceph, ignore it.\n",
            "petrel_client is not installed. Using PIL to load images.\n",
            "FlashAttention2 is not installed.\n",
            "petrel_client is not installed. If you read data locally instead of from ceph, ignore it.\n",
            "petrel_client is not installed. Using PIL to load images.\n",
            "FlashAttention2 is not installed.\n",
            "petrel_client is not installed. If you read data locally instead of from ceph, ignore it.\n",
            "petrel_client is not installed. Using PIL to load images.\n",
            "FlashAttention2 is not installed.\n",
            "petrel_client is not installed. If you read data locally instead of from ceph, ignore it.\n",
            "petrel_client is not installed. Using PIL to load images.\n",
            "dynamic ViT batch size: 19, images per sample: 1.0, dynamic token length: 64\n",
            "[2025-07-31 17:45:10,896] [INFO] [logging.py:107:log_dist] [Rank 0] time (ms) | fwd_microstep: 493.25 | bwd_microstep: 919.17 | bwd_inner_microstep: 919.13 | bwd_allreduce_microstep: 0.01 | step_microstep: 0.01\n",
            "{'loss': 2.6878, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "100%|██████████| 1/1 [00:49<00:00, 48.98s/it][INFO|trainer.py:1962] 2025-07-31 17:45:10,951 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 49.0296, 'train_samples_per_second': 1.04, 'train_steps_per_second': 0.02, 'train_loss': 2.687800884246826, 'epoch': 1.0}\n",
            "100%|██████████| 1/1 [00:49<00:00, 49.02s/it]\n",
            "[INFO|trainer.py:2936] 2025-07-31 17:45:11,843 >> Saving model checkpoint to /content/drive/MyDrive/finetuning-output-Take1\n",
            "[INFO|configuration_utils.py:473] 2025-07-31 17:45:11,848 >> Configuration saved in /content/drive/MyDrive/finetuning-output-Take1/config.json\n",
            "[INFO|configuration_utils.py:594] 2025-07-31 17:45:11,852 >> Configuration saved in /content/drive/MyDrive/finetuning-output-Take1/generation_config.json\n",
            "[INFO|modeling_utils.py:2493] 2025-07-31 17:45:16,181 >> Model weights saved in /content/drive/MyDrive/finetuning-output-Take1/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2433] 2025-07-31 17:45:20,116 >> tokenizer config file saved in /content/drive/MyDrive/finetuning-output-Take1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2442] 2025-07-31 17:45:20,120 >> Special tokens file saved in /content/drive/MyDrive/finetuning-output-Take1/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2493] 2025-07-31 17:45:20,123 >> added tokens file saved in /content/drive/MyDrive/finetuning-output-Take1/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     2.6878\n",
            "  train_runtime            = 0:00:49.02\n",
            "  train_samples            =         51\n",
            "  train_samples_per_second =       1.04\n",
            "  train_steps_per_second   =       0.02\n",
            "[rank0]:[W731 17:45:21.727608627 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "# Using 8 GPUs, fine-tune the full LLM, cost about 30G per GPU\n",
        "%cd /content/VLM-Finetuning-PSL/internvl/internvl_chat\n",
        "!GPUS=1 PER_DEVICE_BATCH_SIZE=32 sh /content/VLM-Finetuning-PSL/internvl/internvl_chat/shell/internvl3.0/2nd_finetune/internvl3_1b_dynamic_res_2nd_finetune_full.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9zGVSXML3cq"
      },
      "source": [
        "Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvtHkjRmSfKh",
        "outputId": "d952b61b-5f6d-4d3c-a180-c68c2c4a2feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: transformers\n",
            "Version: 4.37.2\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers\n"
          ]
        }
      ],
      "source": [
        "!pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB70CioHemsF",
        "outputId": "c7807a41-f6a0-4f6f-f9da-d7f205cda8c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (15.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.37.2)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Using cached transformers-4.54.1-py3-none-any.whl (11.2 MB)\n",
            "Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.37.2\n",
            "    Uninstalling transformers-4.37.2:\n",
            "      Successfully uninstalled transformers-4.37.2\n",
            "Successfully installed tokenizers-0.21.4 transformers-4.54.1\n"
          ]
        }
      ],
      "source": [
        "!pip install av\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "032ddac33ed74c44936faa8efb58581d",
            "86b54d76fed54e768b3c7e75a0d0aa26",
            "d4e061c8936c48dba2632b76c1f8dac6",
            "3264c0adb23041cb893f2fb76c99f921",
            "22bff78435124e2f94731e88558b3057",
            "db062d52a0f247a294f84c12bc5accda",
            "1ec0691a84c44be099025e4f2cf9c5da",
            "67a12bcb216041d597e5f6658c618e17",
            "8abb1b020c02450ebc95d8a16ba2b627",
            "3fa39e9436494d94adff80e0758beb6d",
            "455469308fb64c6d953773efb5c0ea0d"
          ]
        },
        "id": "08_wPaJdCbCc",
        "outputId": "2005fc19-de22-4e60-8549-16ee82139a9e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "032ddac33ed74c44936faa8efb58581d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The word that the person signs in this video is \"please\".'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_checkpoint = \"OpenGVLab/InternVL3-8B-hf\"\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=False)\n",
        "processor = AutoProcessor.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, quantization_config=quantization_config)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"video\",\n",
        "                \"url\": \"/content/drive/MyDrive/PSL_Data_segmentation/Words/you.mp4\",\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": \"Which word does the person sign in this video?\"},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages,\n",
        "    return_tensors=\"pt\",\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    num_frames=8,\n",
        ").to(model.device, dtype=torch.float16)\n",
        "\n",
        "output = model.generate(**inputs, max_new_tokens=25)\n",
        "\n",
        "decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n",
        "decoded_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "GXsaBuudFChY",
        "outputId": "9d88304d-6299-4390-f70a-954aab1718f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/VLM-Finetuning-PSL/internvl/internvl_chat\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/VLM-Finetuning-PSL/internvl/internvl_chat'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd /content/VLM-Finetuning-PSL/internvl/internvl_chat\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T67NKRufEde4",
        "outputId": "5c65ee6f-dffc-4f50-91f8-ddd99214f8a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/VLM-Finetuning-PSL/internvl/internvl_chat\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FlashAttention2 is not installed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/finetuning-output-Take1 were not used when initializing InternVLChatModel: ['language_model.base_model.model.lm_head.weight', 'language_model.base_model.model.model.embed_tokens.weight', 'language_model.base_model.model.model.layers.0.input_layernorm.weight', 'language_model.base_model.model.model.layers.0.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.0.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.input_layernorm.weight', 'language_model.base_model.model.model.layers.1.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.1.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.input_layernorm.weight', 'language_model.base_model.model.model.layers.10.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.10.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.input_layernorm.weight', 'language_model.base_model.model.model.layers.11.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.11.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.input_layernorm.weight', 'language_model.base_model.model.model.layers.12.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.12.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.input_layernorm.weight', 'language_model.base_model.model.model.layers.13.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.13.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.input_layernorm.weight', 'language_model.base_model.model.model.layers.14.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.14.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.input_layernorm.weight', 'language_model.base_model.model.model.layers.15.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.15.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.input_layernorm.weight', 'language_model.base_model.model.model.layers.16.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.16.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.input_layernorm.weight', 'language_model.base_model.model.model.layers.17.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.17.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.input_layernorm.weight', 'language_model.base_model.model.model.layers.18.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.18.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.input_layernorm.weight', 'language_model.base_model.model.model.layers.19.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.19.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.input_layernorm.weight', 'language_model.base_model.model.model.layers.2.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.2.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.input_layernorm.weight', 'language_model.base_model.model.model.layers.20.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.20.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.input_layernorm.weight', 'language_model.base_model.model.model.layers.21.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.21.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.input_layernorm.weight', 'language_model.base_model.model.model.layers.22.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.22.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.input_layernorm.weight', 'language_model.base_model.model.model.layers.23.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.23.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.input_layernorm.weight', 'language_model.base_model.model.model.layers.3.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.3.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.input_layernorm.weight', 'language_model.base_model.model.model.layers.4.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.4.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.input_layernorm.weight', 'language_model.base_model.model.model.layers.5.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.5.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.input_layernorm.weight', 'language_model.base_model.model.model.layers.6.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.6.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.input_layernorm.weight', 'language_model.base_model.model.model.layers.7.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.7.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.input_layernorm.weight', 'language_model.base_model.model.model.layers.8.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.8.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.input_layernorm.weight', 'language_model.base_model.model.model.layers.9.mlp.down_proj.base_layer.weight', 'language_model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight', 'language_model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.mlp.up_proj.base_layer.weight', 'language_model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'language_model.base_model.model.model.layers.9.self_attn.k_proj.base_layer.bias', 'language_model.base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight', 'language_model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight', 'language_model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.self_attn.q_proj.base_layer.bias', 'language_model.base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight', 'language_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.self_attn.v_proj.base_layer.bias', 'language_model.base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight', 'language_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'language_model.base_model.model.model.norm.weight', 'vision_model.base_model.model.embeddings.class_embedding', 'vision_model.base_model.model.embeddings.patch_embedding.bias', 'vision_model.base_model.model.embeddings.patch_embedding.weight', 'vision_model.base_model.model.embeddings.position_embedding', 'vision_model.base_model.model.encoder.layers.0.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.0.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.0.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.0.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.0.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.0.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.0.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.0.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.0.ls1', 'vision_model.base_model.model.encoder.layers.0.ls2', 'vision_model.base_model.model.encoder.layers.0.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.0.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.0.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.0.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.0.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.0.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.0.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.0.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.0.norm1.bias', 'vision_model.base_model.model.encoder.layers.0.norm1.weight', 'vision_model.base_model.model.encoder.layers.0.norm2.bias', 'vision_model.base_model.model.encoder.layers.0.norm2.weight', 'vision_model.base_model.model.encoder.layers.1.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.1.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.1.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.1.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.1.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.1.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.1.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.1.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.1.ls1', 'vision_model.base_model.model.encoder.layers.1.ls2', 'vision_model.base_model.model.encoder.layers.1.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.1.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.1.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.1.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.1.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.1.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.1.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.1.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.1.norm1.bias', 'vision_model.base_model.model.encoder.layers.1.norm1.weight', 'vision_model.base_model.model.encoder.layers.1.norm2.bias', 'vision_model.base_model.model.encoder.layers.1.norm2.weight', 'vision_model.base_model.model.encoder.layers.10.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.10.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.10.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.10.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.10.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.10.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.10.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.10.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.10.ls1', 'vision_model.base_model.model.encoder.layers.10.ls2', 'vision_model.base_model.model.encoder.layers.10.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.10.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.10.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.10.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.10.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.10.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.10.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.10.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.10.norm1.bias', 'vision_model.base_model.model.encoder.layers.10.norm1.weight', 'vision_model.base_model.model.encoder.layers.10.norm2.bias', 'vision_model.base_model.model.encoder.layers.10.norm2.weight', 'vision_model.base_model.model.encoder.layers.11.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.11.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.11.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.11.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.11.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.11.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.11.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.11.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.11.ls1', 'vision_model.base_model.model.encoder.layers.11.ls2', 'vision_model.base_model.model.encoder.layers.11.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.11.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.11.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.11.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.11.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.11.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.11.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.11.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.11.norm1.bias', 'vision_model.base_model.model.encoder.layers.11.norm1.weight', 'vision_model.base_model.model.encoder.layers.11.norm2.bias', 'vision_model.base_model.model.encoder.layers.11.norm2.weight', 'vision_model.base_model.model.encoder.layers.12.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.12.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.12.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.12.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.12.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.12.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.12.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.12.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.12.ls1', 'vision_model.base_model.model.encoder.layers.12.ls2', 'vision_model.base_model.model.encoder.layers.12.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.12.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.12.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.12.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.12.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.12.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.12.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.12.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.12.norm1.bias', 'vision_model.base_model.model.encoder.layers.12.norm1.weight', 'vision_model.base_model.model.encoder.layers.12.norm2.bias', 'vision_model.base_model.model.encoder.layers.12.norm2.weight', 'vision_model.base_model.model.encoder.layers.13.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.13.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.13.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.13.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.13.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.13.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.13.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.13.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.13.ls1', 'vision_model.base_model.model.encoder.layers.13.ls2', 'vision_model.base_model.model.encoder.layers.13.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.13.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.13.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.13.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.13.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.13.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.13.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.13.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.13.norm1.bias', 'vision_model.base_model.model.encoder.layers.13.norm1.weight', 'vision_model.base_model.model.encoder.layers.13.norm2.bias', 'vision_model.base_model.model.encoder.layers.13.norm2.weight', 'vision_model.base_model.model.encoder.layers.14.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.14.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.14.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.14.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.14.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.14.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.14.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.14.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.14.ls1', 'vision_model.base_model.model.encoder.layers.14.ls2', 'vision_model.base_model.model.encoder.layers.14.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.14.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.14.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.14.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.14.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.14.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.14.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.14.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.14.norm1.bias', 'vision_model.base_model.model.encoder.layers.14.norm1.weight', 'vision_model.base_model.model.encoder.layers.14.norm2.bias', 'vision_model.base_model.model.encoder.layers.14.norm2.weight', 'vision_model.base_model.model.encoder.layers.15.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.15.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.15.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.15.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.15.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.15.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.15.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.15.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.15.ls1', 'vision_model.base_model.model.encoder.layers.15.ls2', 'vision_model.base_model.model.encoder.layers.15.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.15.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.15.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.15.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.15.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.15.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.15.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.15.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.15.norm1.bias', 'vision_model.base_model.model.encoder.layers.15.norm1.weight', 'vision_model.base_model.model.encoder.layers.15.norm2.bias', 'vision_model.base_model.model.encoder.layers.15.norm2.weight', 'vision_model.base_model.model.encoder.layers.16.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.16.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.16.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.16.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.16.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.16.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.16.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.16.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.16.ls1', 'vision_model.base_model.model.encoder.layers.16.ls2', 'vision_model.base_model.model.encoder.layers.16.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.16.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.16.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.16.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.16.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.16.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.16.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.16.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.16.norm1.bias', 'vision_model.base_model.model.encoder.layers.16.norm1.weight', 'vision_model.base_model.model.encoder.layers.16.norm2.bias', 'vision_model.base_model.model.encoder.layers.16.norm2.weight', 'vision_model.base_model.model.encoder.layers.17.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.17.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.17.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.17.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.17.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.17.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.17.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.17.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.17.ls1', 'vision_model.base_model.model.encoder.layers.17.ls2', 'vision_model.base_model.model.encoder.layers.17.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.17.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.17.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.17.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.17.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.17.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.17.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.17.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.17.norm1.bias', 'vision_model.base_model.model.encoder.layers.17.norm1.weight', 'vision_model.base_model.model.encoder.layers.17.norm2.bias', 'vision_model.base_model.model.encoder.layers.17.norm2.weight', 'vision_model.base_model.model.encoder.layers.18.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.18.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.18.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.18.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.18.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.18.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.18.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.18.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.18.ls1', 'vision_model.base_model.model.encoder.layers.18.ls2', 'vision_model.base_model.model.encoder.layers.18.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.18.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.18.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.18.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.18.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.18.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.18.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.18.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.18.norm1.bias', 'vision_model.base_model.model.encoder.layers.18.norm1.weight', 'vision_model.base_model.model.encoder.layers.18.norm2.bias', 'vision_model.base_model.model.encoder.layers.18.norm2.weight', 'vision_model.base_model.model.encoder.layers.19.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.19.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.19.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.19.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.19.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.19.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.19.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.19.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.19.ls1', 'vision_model.base_model.model.encoder.layers.19.ls2', 'vision_model.base_model.model.encoder.layers.19.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.19.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.19.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.19.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.19.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.19.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.19.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.19.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.19.norm1.bias', 'vision_model.base_model.model.encoder.layers.19.norm1.weight', 'vision_model.base_model.model.encoder.layers.19.norm2.bias', 'vision_model.base_model.model.encoder.layers.19.norm2.weight', 'vision_model.base_model.model.encoder.layers.2.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.2.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.2.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.2.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.2.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.2.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.2.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.2.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.2.ls1', 'vision_model.base_model.model.encoder.layers.2.ls2', 'vision_model.base_model.model.encoder.layers.2.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.2.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.2.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.2.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.2.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.2.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.2.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.2.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.2.norm1.bias', 'vision_model.base_model.model.encoder.layers.2.norm1.weight', 'vision_model.base_model.model.encoder.layers.2.norm2.bias', 'vision_model.base_model.model.encoder.layers.2.norm2.weight', 'vision_model.base_model.model.encoder.layers.20.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.20.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.20.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.20.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.20.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.20.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.20.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.20.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.20.ls1', 'vision_model.base_model.model.encoder.layers.20.ls2', 'vision_model.base_model.model.encoder.layers.20.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.20.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.20.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.20.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.20.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.20.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.20.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.20.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.20.norm1.bias', 'vision_model.base_model.model.encoder.layers.20.norm1.weight', 'vision_model.base_model.model.encoder.layers.20.norm2.bias', 'vision_model.base_model.model.encoder.layers.20.norm2.weight', 'vision_model.base_model.model.encoder.layers.21.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.21.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.21.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.21.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.21.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.21.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.21.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.21.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.21.ls1', 'vision_model.base_model.model.encoder.layers.21.ls2', 'vision_model.base_model.model.encoder.layers.21.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.21.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.21.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.21.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.21.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.21.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.21.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.21.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.21.norm1.bias', 'vision_model.base_model.model.encoder.layers.21.norm1.weight', 'vision_model.base_model.model.encoder.layers.21.norm2.bias', 'vision_model.base_model.model.encoder.layers.21.norm2.weight', 'vision_model.base_model.model.encoder.layers.22.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.22.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.22.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.22.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.22.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.22.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.22.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.22.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.22.ls1', 'vision_model.base_model.model.encoder.layers.22.ls2', 'vision_model.base_model.model.encoder.layers.22.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.22.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.22.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.22.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.22.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.22.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.22.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.22.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.22.norm1.bias', 'vision_model.base_model.model.encoder.layers.22.norm1.weight', 'vision_model.base_model.model.encoder.layers.22.norm2.bias', 'vision_model.base_model.model.encoder.layers.22.norm2.weight', 'vision_model.base_model.model.encoder.layers.23.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.23.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.23.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.23.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.23.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.23.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.23.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.23.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.23.ls1', 'vision_model.base_model.model.encoder.layers.23.ls2', 'vision_model.base_model.model.encoder.layers.23.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.23.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.23.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.23.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.23.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.23.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.23.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.23.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.23.norm1.bias', 'vision_model.base_model.model.encoder.layers.23.norm1.weight', 'vision_model.base_model.model.encoder.layers.23.norm2.bias', 'vision_model.base_model.model.encoder.layers.23.norm2.weight', 'vision_model.base_model.model.encoder.layers.3.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.3.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.3.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.3.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.3.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.3.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.3.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.3.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.3.ls1', 'vision_model.base_model.model.encoder.layers.3.ls2', 'vision_model.base_model.model.encoder.layers.3.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.3.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.3.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.3.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.3.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.3.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.3.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.3.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.3.norm1.bias', 'vision_model.base_model.model.encoder.layers.3.norm1.weight', 'vision_model.base_model.model.encoder.layers.3.norm2.bias', 'vision_model.base_model.model.encoder.layers.3.norm2.weight', 'vision_model.base_model.model.encoder.layers.4.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.4.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.4.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.4.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.4.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.4.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.4.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.4.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.4.ls1', 'vision_model.base_model.model.encoder.layers.4.ls2', 'vision_model.base_model.model.encoder.layers.4.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.4.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.4.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.4.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.4.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.4.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.4.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.4.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.4.norm1.bias', 'vision_model.base_model.model.encoder.layers.4.norm1.weight', 'vision_model.base_model.model.encoder.layers.4.norm2.bias', 'vision_model.base_model.model.encoder.layers.4.norm2.weight', 'vision_model.base_model.model.encoder.layers.5.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.5.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.5.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.5.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.5.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.5.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.5.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.5.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.5.ls1', 'vision_model.base_model.model.encoder.layers.5.ls2', 'vision_model.base_model.model.encoder.layers.5.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.5.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.5.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.5.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.5.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.5.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.5.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.5.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.5.norm1.bias', 'vision_model.base_model.model.encoder.layers.5.norm1.weight', 'vision_model.base_model.model.encoder.layers.5.norm2.bias', 'vision_model.base_model.model.encoder.layers.5.norm2.weight', 'vision_model.base_model.model.encoder.layers.6.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.6.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.6.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.6.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.6.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.6.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.6.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.6.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.6.ls1', 'vision_model.base_model.model.encoder.layers.6.ls2', 'vision_model.base_model.model.encoder.layers.6.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.6.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.6.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.6.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.6.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.6.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.6.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.6.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.6.norm1.bias', 'vision_model.base_model.model.encoder.layers.6.norm1.weight', 'vision_model.base_model.model.encoder.layers.6.norm2.bias', 'vision_model.base_model.model.encoder.layers.6.norm2.weight', 'vision_model.base_model.model.encoder.layers.7.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.7.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.7.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.7.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.7.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.7.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.7.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.7.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.7.ls1', 'vision_model.base_model.model.encoder.layers.7.ls2', 'vision_model.base_model.model.encoder.layers.7.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.7.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.7.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.7.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.7.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.7.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.7.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.7.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.7.norm1.bias', 'vision_model.base_model.model.encoder.layers.7.norm1.weight', 'vision_model.base_model.model.encoder.layers.7.norm2.bias', 'vision_model.base_model.model.encoder.layers.7.norm2.weight', 'vision_model.base_model.model.encoder.layers.8.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.8.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.8.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.8.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.8.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.8.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.8.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.8.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.8.ls1', 'vision_model.base_model.model.encoder.layers.8.ls2', 'vision_model.base_model.model.encoder.layers.8.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.8.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.8.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.8.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.8.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.8.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.8.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.8.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.8.norm1.bias', 'vision_model.base_model.model.encoder.layers.8.norm1.weight', 'vision_model.base_model.model.encoder.layers.8.norm2.bias', 'vision_model.base_model.model.encoder.layers.8.norm2.weight', 'vision_model.base_model.model.encoder.layers.9.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.9.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.9.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.9.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.9.attn.qkv.base_layer.bias', 'vision_model.base_model.model.encoder.layers.9.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.9.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.9.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.9.ls1', 'vision_model.base_model.model.encoder.layers.9.ls2', 'vision_model.base_model.model.encoder.layers.9.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.9.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.9.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.9.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.9.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.9.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.9.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.9.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.9.norm1.bias', 'vision_model.base_model.model.encoder.layers.9.norm1.weight', 'vision_model.base_model.model.encoder.layers.9.norm2.bias', 'vision_model.base_model.model.encoder.layers.9.norm2.weight']\n",
            "- This IS expected if you are initializing InternVLChatModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing InternVLChatModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of InternVLChatModel were not initialized from the model checkpoint at /content/drive/MyDrive/finetuning-output-Take1 and are newly initialized: ['language_model.lm_head.weight', 'language_model.model.embed_tokens.weight', 'language_model.model.layers.0.input_layernorm.weight', 'language_model.model.layers.0.mlp.down_proj.weight', 'language_model.model.layers.0.mlp.gate_proj.weight', 'language_model.model.layers.0.mlp.up_proj.weight', 'language_model.model.layers.0.post_attention_layernorm.weight', 'language_model.model.layers.0.self_attn.k_proj.bias', 'language_model.model.layers.0.self_attn.k_proj.weight', 'language_model.model.layers.0.self_attn.o_proj.weight', 'language_model.model.layers.0.self_attn.q_proj.bias', 'language_model.model.layers.0.self_attn.q_proj.weight', 'language_model.model.layers.0.self_attn.v_proj.bias', 'language_model.model.layers.0.self_attn.v_proj.weight', 'language_model.model.layers.1.input_layernorm.weight', 'language_model.model.layers.1.mlp.down_proj.weight', 'language_model.model.layers.1.mlp.gate_proj.weight', 'language_model.model.layers.1.mlp.up_proj.weight', 'language_model.model.layers.1.post_attention_layernorm.weight', 'language_model.model.layers.1.self_attn.k_proj.bias', 'language_model.model.layers.1.self_attn.k_proj.weight', 'language_model.model.layers.1.self_attn.o_proj.weight', 'language_model.model.layers.1.self_attn.q_proj.bias', 'language_model.model.layers.1.self_attn.q_proj.weight', 'language_model.model.layers.1.self_attn.v_proj.bias', 'language_model.model.layers.1.self_attn.v_proj.weight', 'language_model.model.layers.10.input_layernorm.weight', 'language_model.model.layers.10.mlp.down_proj.weight', 'language_model.model.layers.10.mlp.gate_proj.weight', 'language_model.model.layers.10.mlp.up_proj.weight', 'language_model.model.layers.10.post_attention_layernorm.weight', 'language_model.model.layers.10.self_attn.k_proj.bias', 'language_model.model.layers.10.self_attn.k_proj.weight', 'language_model.model.layers.10.self_attn.o_proj.weight', 'language_model.model.layers.10.self_attn.q_proj.bias', 'language_model.model.layers.10.self_attn.q_proj.weight', 'language_model.model.layers.10.self_attn.v_proj.bias', 'language_model.model.layers.10.self_attn.v_proj.weight', 'language_model.model.layers.11.input_layernorm.weight', 'language_model.model.layers.11.mlp.down_proj.weight', 'language_model.model.layers.11.mlp.gate_proj.weight', 'language_model.model.layers.11.mlp.up_proj.weight', 'language_model.model.layers.11.post_attention_layernorm.weight', 'language_model.model.layers.11.self_attn.k_proj.bias', 'language_model.model.layers.11.self_attn.k_proj.weight', 'language_model.model.layers.11.self_attn.o_proj.weight', 'language_model.model.layers.11.self_attn.q_proj.bias', 'language_model.model.layers.11.self_attn.q_proj.weight', 'language_model.model.layers.11.self_attn.v_proj.bias', 'language_model.model.layers.11.self_attn.v_proj.weight', 'language_model.model.layers.12.input_layernorm.weight', 'language_model.model.layers.12.mlp.down_proj.weight', 'language_model.model.layers.12.mlp.gate_proj.weight', 'language_model.model.layers.12.mlp.up_proj.weight', 'language_model.model.layers.12.post_attention_layernorm.weight', 'language_model.model.layers.12.self_attn.k_proj.bias', 'language_model.model.layers.12.self_attn.k_proj.weight', 'language_model.model.layers.12.self_attn.o_proj.weight', 'language_model.model.layers.12.self_attn.q_proj.bias', 'language_model.model.layers.12.self_attn.q_proj.weight', 'language_model.model.layers.12.self_attn.v_proj.bias', 'language_model.model.layers.12.self_attn.v_proj.weight', 'language_model.model.layers.13.input_layernorm.weight', 'language_model.model.layers.13.mlp.down_proj.weight', 'language_model.model.layers.13.mlp.gate_proj.weight', 'language_model.model.layers.13.mlp.up_proj.weight', 'language_model.model.layers.13.post_attention_layernorm.weight', 'language_model.model.layers.13.self_attn.k_proj.bias', 'language_model.model.layers.13.self_attn.k_proj.weight', 'language_model.model.layers.13.self_attn.o_proj.weight', 'language_model.model.layers.13.self_attn.q_proj.bias', 'language_model.model.layers.13.self_attn.q_proj.weight', 'language_model.model.layers.13.self_attn.v_proj.bias', 'language_model.model.layers.13.self_attn.v_proj.weight', 'language_model.model.layers.14.input_layernorm.weight', 'language_model.model.layers.14.mlp.down_proj.weight', 'language_model.model.layers.14.mlp.gate_proj.weight', 'language_model.model.layers.14.mlp.up_proj.weight', 'language_model.model.layers.14.post_attention_layernorm.weight', 'language_model.model.layers.14.self_attn.k_proj.bias', 'language_model.model.layers.14.self_attn.k_proj.weight', 'language_model.model.layers.14.self_attn.o_proj.weight', 'language_model.model.layers.14.self_attn.q_proj.bias', 'language_model.model.layers.14.self_attn.q_proj.weight', 'language_model.model.layers.14.self_attn.v_proj.bias', 'language_model.model.layers.14.self_attn.v_proj.weight', 'language_model.model.layers.15.input_layernorm.weight', 'language_model.model.layers.15.mlp.down_proj.weight', 'language_model.model.layers.15.mlp.gate_proj.weight', 'language_model.model.layers.15.mlp.up_proj.weight', 'language_model.model.layers.15.post_attention_layernorm.weight', 'language_model.model.layers.15.self_attn.k_proj.bias', 'language_model.model.layers.15.self_attn.k_proj.weight', 'language_model.model.layers.15.self_attn.o_proj.weight', 'language_model.model.layers.15.self_attn.q_proj.bias', 'language_model.model.layers.15.self_attn.q_proj.weight', 'language_model.model.layers.15.self_attn.v_proj.bias', 'language_model.model.layers.15.self_attn.v_proj.weight', 'language_model.model.layers.16.input_layernorm.weight', 'language_model.model.layers.16.mlp.down_proj.weight', 'language_model.model.layers.16.mlp.gate_proj.weight', 'language_model.model.layers.16.mlp.up_proj.weight', 'language_model.model.layers.16.post_attention_layernorm.weight', 'language_model.model.layers.16.self_attn.k_proj.bias', 'language_model.model.layers.16.self_attn.k_proj.weight', 'language_model.model.layers.16.self_attn.o_proj.weight', 'language_model.model.layers.16.self_attn.q_proj.bias', 'language_model.model.layers.16.self_attn.q_proj.weight', 'language_model.model.layers.16.self_attn.v_proj.bias', 'language_model.model.layers.16.self_attn.v_proj.weight', 'language_model.model.layers.17.input_layernorm.weight', 'language_model.model.layers.17.mlp.down_proj.weight', 'language_model.model.layers.17.mlp.gate_proj.weight', 'language_model.model.layers.17.mlp.up_proj.weight', 'language_model.model.layers.17.post_attention_layernorm.weight', 'language_model.model.layers.17.self_attn.k_proj.bias', 'language_model.model.layers.17.self_attn.k_proj.weight', 'language_model.model.layers.17.self_attn.o_proj.weight', 'language_model.model.layers.17.self_attn.q_proj.bias', 'language_model.model.layers.17.self_attn.q_proj.weight', 'language_model.model.layers.17.self_attn.v_proj.bias', 'language_model.model.layers.17.self_attn.v_proj.weight', 'language_model.model.layers.18.input_layernorm.weight', 'language_model.model.layers.18.mlp.down_proj.weight', 'language_model.model.layers.18.mlp.gate_proj.weight', 'language_model.model.layers.18.mlp.up_proj.weight', 'language_model.model.layers.18.post_attention_layernorm.weight', 'language_model.model.layers.18.self_attn.k_proj.bias', 'language_model.model.layers.18.self_attn.k_proj.weight', 'language_model.model.layers.18.self_attn.o_proj.weight', 'language_model.model.layers.18.self_attn.q_proj.bias', 'language_model.model.layers.18.self_attn.q_proj.weight', 'language_model.model.layers.18.self_attn.v_proj.bias', 'language_model.model.layers.18.self_attn.v_proj.weight', 'language_model.model.layers.19.input_layernorm.weight', 'language_model.model.layers.19.mlp.down_proj.weight', 'language_model.model.layers.19.mlp.gate_proj.weight', 'language_model.model.layers.19.mlp.up_proj.weight', 'language_model.model.layers.19.post_attention_layernorm.weight', 'language_model.model.layers.19.self_attn.k_proj.bias', 'language_model.model.layers.19.self_attn.k_proj.weight', 'language_model.model.layers.19.self_attn.o_proj.weight', 'language_model.model.layers.19.self_attn.q_proj.bias', 'language_model.model.layers.19.self_attn.q_proj.weight', 'language_model.model.layers.19.self_attn.v_proj.bias', 'language_model.model.layers.19.self_attn.v_proj.weight', 'language_model.model.layers.2.input_layernorm.weight', 'language_model.model.layers.2.mlp.down_proj.weight', 'language_model.model.layers.2.mlp.gate_proj.weight', 'language_model.model.layers.2.mlp.up_proj.weight', 'language_model.model.layers.2.post_attention_layernorm.weight', 'language_model.model.layers.2.self_attn.k_proj.bias', 'language_model.model.layers.2.self_attn.k_proj.weight', 'language_model.model.layers.2.self_attn.o_proj.weight', 'language_model.model.layers.2.self_attn.q_proj.bias', 'language_model.model.layers.2.self_attn.q_proj.weight', 'language_model.model.layers.2.self_attn.v_proj.bias', 'language_model.model.layers.2.self_attn.v_proj.weight', 'language_model.model.layers.20.input_layernorm.weight', 'language_model.model.layers.20.mlp.down_proj.weight', 'language_model.model.layers.20.mlp.gate_proj.weight', 'language_model.model.layers.20.mlp.up_proj.weight', 'language_model.model.layers.20.post_attention_layernorm.weight', 'language_model.model.layers.20.self_attn.k_proj.bias', 'language_model.model.layers.20.self_attn.k_proj.weight', 'language_model.model.layers.20.self_attn.o_proj.weight', 'language_model.model.layers.20.self_attn.q_proj.bias', 'language_model.model.layers.20.self_attn.q_proj.weight', 'language_model.model.layers.20.self_attn.v_proj.bias', 'language_model.model.layers.20.self_attn.v_proj.weight', 'language_model.model.layers.21.input_layernorm.weight', 'language_model.model.layers.21.mlp.down_proj.weight', 'language_model.model.layers.21.mlp.gate_proj.weight', 'language_model.model.layers.21.mlp.up_proj.weight', 'language_model.model.layers.21.post_attention_layernorm.weight', 'language_model.model.layers.21.self_attn.k_proj.bias', 'language_model.model.layers.21.self_attn.k_proj.weight', 'language_model.model.layers.21.self_attn.o_proj.weight', 'language_model.model.layers.21.self_attn.q_proj.bias', 'language_model.model.layers.21.self_attn.q_proj.weight', 'language_model.model.layers.21.self_attn.v_proj.bias', 'language_model.model.layers.21.self_attn.v_proj.weight', 'language_model.model.layers.22.input_layernorm.weight', 'language_model.model.layers.22.mlp.down_proj.weight', 'language_model.model.layers.22.mlp.gate_proj.weight', 'language_model.model.layers.22.mlp.up_proj.weight', 'language_model.model.layers.22.post_attention_layernorm.weight', 'language_model.model.layers.22.self_attn.k_proj.bias', 'language_model.model.layers.22.self_attn.k_proj.weight', 'language_model.model.layers.22.self_attn.o_proj.weight', 'language_model.model.layers.22.self_attn.q_proj.bias', 'language_model.model.layers.22.self_attn.q_proj.weight', 'language_model.model.layers.22.self_attn.v_proj.bias', 'language_model.model.layers.22.self_attn.v_proj.weight', 'language_model.model.layers.23.input_layernorm.weight', 'language_model.model.layers.23.mlp.down_proj.weight', 'language_model.model.layers.23.mlp.gate_proj.weight', 'language_model.model.layers.23.mlp.up_proj.weight', 'language_model.model.layers.23.post_attention_layernorm.weight', 'language_model.model.layers.23.self_attn.k_proj.bias', 'language_model.model.layers.23.self_attn.k_proj.weight', 'language_model.model.layers.23.self_attn.o_proj.weight', 'language_model.model.layers.23.self_attn.q_proj.bias', 'language_model.model.layers.23.self_attn.q_proj.weight', 'language_model.model.layers.23.self_attn.v_proj.bias', 'language_model.model.layers.23.self_attn.v_proj.weight', 'language_model.model.layers.3.input_layernorm.weight', 'language_model.model.layers.3.mlp.down_proj.weight', 'language_model.model.layers.3.mlp.gate_proj.weight', 'language_model.model.layers.3.mlp.up_proj.weight', 'language_model.model.layers.3.post_attention_layernorm.weight', 'language_model.model.layers.3.self_attn.k_proj.bias', 'language_model.model.layers.3.self_attn.k_proj.weight', 'language_model.model.layers.3.self_attn.o_proj.weight', 'language_model.model.layers.3.self_attn.q_proj.bias', 'language_model.model.layers.3.self_attn.q_proj.weight', 'language_model.model.layers.3.self_attn.v_proj.bias', 'language_model.model.layers.3.self_attn.v_proj.weight', 'language_model.model.layers.4.input_layernorm.weight', 'language_model.model.layers.4.mlp.down_proj.weight', 'language_model.model.layers.4.mlp.gate_proj.weight', 'language_model.model.layers.4.mlp.up_proj.weight', 'language_model.model.layers.4.post_attention_layernorm.weight', 'language_model.model.layers.4.self_attn.k_proj.bias', 'language_model.model.layers.4.self_attn.k_proj.weight', 'language_model.model.layers.4.self_attn.o_proj.weight', 'language_model.model.layers.4.self_attn.q_proj.bias', 'language_model.model.layers.4.self_attn.q_proj.weight', 'language_model.model.layers.4.self_attn.v_proj.bias', 'language_model.model.layers.4.self_attn.v_proj.weight', 'language_model.model.layers.5.input_layernorm.weight', 'language_model.model.layers.5.mlp.down_proj.weight', 'language_model.model.layers.5.mlp.gate_proj.weight', 'language_model.model.layers.5.mlp.up_proj.weight', 'language_model.model.layers.5.post_attention_layernorm.weight', 'language_model.model.layers.5.self_attn.k_proj.bias', 'language_model.model.layers.5.self_attn.k_proj.weight', 'language_model.model.layers.5.self_attn.o_proj.weight', 'language_model.model.layers.5.self_attn.q_proj.bias', 'language_model.model.layers.5.self_attn.q_proj.weight', 'language_model.model.layers.5.self_attn.v_proj.bias', 'language_model.model.layers.5.self_attn.v_proj.weight', 'language_model.model.layers.6.input_layernorm.weight', 'language_model.model.layers.6.mlp.down_proj.weight', 'language_model.model.layers.6.mlp.gate_proj.weight', 'language_model.model.layers.6.mlp.up_proj.weight', 'language_model.model.layers.6.post_attention_layernorm.weight', 'language_model.model.layers.6.self_attn.k_proj.bias', 'language_model.model.layers.6.self_attn.k_proj.weight', 'language_model.model.layers.6.self_attn.o_proj.weight', 'language_model.model.layers.6.self_attn.q_proj.bias', 'language_model.model.layers.6.self_attn.q_proj.weight', 'language_model.model.layers.6.self_attn.v_proj.bias', 'language_model.model.layers.6.self_attn.v_proj.weight', 'language_model.model.layers.7.input_layernorm.weight', 'language_model.model.layers.7.mlp.down_proj.weight', 'language_model.model.layers.7.mlp.gate_proj.weight', 'language_model.model.layers.7.mlp.up_proj.weight', 'language_model.model.layers.7.post_attention_layernorm.weight', 'language_model.model.layers.7.self_attn.k_proj.bias', 'language_model.model.layers.7.self_attn.k_proj.weight', 'language_model.model.layers.7.self_attn.o_proj.weight', 'language_model.model.layers.7.self_attn.q_proj.bias', 'language_model.model.layers.7.self_attn.q_proj.weight', 'language_model.model.layers.7.self_attn.v_proj.bias', 'language_model.model.layers.7.self_attn.v_proj.weight', 'language_model.model.layers.8.input_layernorm.weight', 'language_model.model.layers.8.mlp.down_proj.weight', 'language_model.model.layers.8.mlp.gate_proj.weight', 'language_model.model.layers.8.mlp.up_proj.weight', 'language_model.model.layers.8.post_attention_layernorm.weight', 'language_model.model.layers.8.self_attn.k_proj.bias', 'language_model.model.layers.8.self_attn.k_proj.weight', 'language_model.model.layers.8.self_attn.o_proj.weight', 'language_model.model.layers.8.self_attn.q_proj.bias', 'language_model.model.layers.8.self_attn.q_proj.weight', 'language_model.model.layers.8.self_attn.v_proj.bias', 'language_model.model.layers.8.self_attn.v_proj.weight', 'language_model.model.layers.9.input_layernorm.weight', 'language_model.model.layers.9.mlp.down_proj.weight', 'language_model.model.layers.9.mlp.gate_proj.weight', 'language_model.model.layers.9.mlp.up_proj.weight', 'language_model.model.layers.9.post_attention_layernorm.weight', 'language_model.model.layers.9.self_attn.k_proj.bias', 'language_model.model.layers.9.self_attn.k_proj.weight', 'language_model.model.layers.9.self_attn.o_proj.weight', 'language_model.model.layers.9.self_attn.q_proj.bias', 'language_model.model.layers.9.self_attn.q_proj.weight', 'language_model.model.layers.9.self_attn.v_proj.bias', 'language_model.model.layers.9.self_attn.v_proj.weight', 'language_model.model.norm.weight', 'vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.attn.proj.bias', 'vision_model.encoder.layers.0.attn.proj.weight', 'vision_model.encoder.layers.0.attn.qkv.bias', 'vision_model.encoder.layers.0.attn.qkv.weight', 'vision_model.encoder.layers.0.ls1', 'vision_model.encoder.layers.0.ls2', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.norm1.bias', 'vision_model.encoder.layers.0.norm1.weight', 'vision_model.encoder.layers.0.norm2.bias', 'vision_model.encoder.layers.0.norm2.weight', 'vision_model.encoder.layers.1.attn.proj.bias', 'vision_model.encoder.layers.1.attn.proj.weight', 'vision_model.encoder.layers.1.attn.qkv.bias', 'vision_model.encoder.layers.1.attn.qkv.weight', 'vision_model.encoder.layers.1.ls1', 'vision_model.encoder.layers.1.ls2', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.norm1.bias', 'vision_model.encoder.layers.1.norm1.weight', 'vision_model.encoder.layers.1.norm2.bias', 'vision_model.encoder.layers.1.norm2.weight', 'vision_model.encoder.layers.10.attn.proj.bias', 'vision_model.encoder.layers.10.attn.proj.weight', 'vision_model.encoder.layers.10.attn.qkv.bias', 'vision_model.encoder.layers.10.attn.qkv.weight', 'vision_model.encoder.layers.10.ls1', 'vision_model.encoder.layers.10.ls2', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.norm1.bias', 'vision_model.encoder.layers.10.norm1.weight', 'vision_model.encoder.layers.10.norm2.bias', 'vision_model.encoder.layers.10.norm2.weight', 'vision_model.encoder.layers.11.attn.proj.bias', 'vision_model.encoder.layers.11.attn.proj.weight', 'vision_model.encoder.layers.11.attn.qkv.bias', 'vision_model.encoder.layers.11.attn.qkv.weight', 'vision_model.encoder.layers.11.ls1', 'vision_model.encoder.layers.11.ls2', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.norm1.bias', 'vision_model.encoder.layers.11.norm1.weight', 'vision_model.encoder.layers.11.norm2.bias', 'vision_model.encoder.layers.11.norm2.weight', 'vision_model.encoder.layers.12.attn.proj.bias', 'vision_model.encoder.layers.12.attn.proj.weight', 'vision_model.encoder.layers.12.attn.qkv.bias', 'vision_model.encoder.layers.12.attn.qkv.weight', 'vision_model.encoder.layers.12.ls1', 'vision_model.encoder.layers.12.ls2', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.norm1.bias', 'vision_model.encoder.layers.12.norm1.weight', 'vision_model.encoder.layers.12.norm2.bias', 'vision_model.encoder.layers.12.norm2.weight', 'vision_model.encoder.layers.13.attn.proj.bias', 'vision_model.encoder.layers.13.attn.proj.weight', 'vision_model.encoder.layers.13.attn.qkv.bias', 'vision_model.encoder.layers.13.attn.qkv.weight', 'vision_model.encoder.layers.13.ls1', 'vision_model.encoder.layers.13.ls2', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.norm1.bias', 'vision_model.encoder.layers.13.norm1.weight', 'vision_model.encoder.layers.13.norm2.bias', 'vision_model.encoder.layers.13.norm2.weight', 'vision_model.encoder.layers.14.attn.proj.bias', 'vision_model.encoder.layers.14.attn.proj.weight', 'vision_model.encoder.layers.14.attn.qkv.bias', 'vision_model.encoder.layers.14.attn.qkv.weight', 'vision_model.encoder.layers.14.ls1', 'vision_model.encoder.layers.14.ls2', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.norm1.bias', 'vision_model.encoder.layers.14.norm1.weight', 'vision_model.encoder.layers.14.norm2.bias', 'vision_model.encoder.layers.14.norm2.weight', 'vision_model.encoder.layers.15.attn.proj.bias', 'vision_model.encoder.layers.15.attn.proj.weight', 'vision_model.encoder.layers.15.attn.qkv.bias', 'vision_model.encoder.layers.15.attn.qkv.weight', 'vision_model.encoder.layers.15.ls1', 'vision_model.encoder.layers.15.ls2', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.norm1.bias', 'vision_model.encoder.layers.15.norm1.weight', 'vision_model.encoder.layers.15.norm2.bias', 'vision_model.encoder.layers.15.norm2.weight', 'vision_model.encoder.layers.16.attn.proj.bias', 'vision_model.encoder.layers.16.attn.proj.weight', 'vision_model.encoder.layers.16.attn.qkv.bias', 'vision_model.encoder.layers.16.attn.qkv.weight', 'vision_model.encoder.layers.16.ls1', 'vision_model.encoder.layers.16.ls2', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.norm1.bias', 'vision_model.encoder.layers.16.norm1.weight', 'vision_model.encoder.layers.16.norm2.bias', 'vision_model.encoder.layers.16.norm2.weight', 'vision_model.encoder.layers.17.attn.proj.bias', 'vision_model.encoder.layers.17.attn.proj.weight', 'vision_model.encoder.layers.17.attn.qkv.bias', 'vision_model.encoder.layers.17.attn.qkv.weight', 'vision_model.encoder.layers.17.ls1', 'vision_model.encoder.layers.17.ls2', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.norm1.bias', 'vision_model.encoder.layers.17.norm1.weight', 'vision_model.encoder.layers.17.norm2.bias', 'vision_model.encoder.layers.17.norm2.weight', 'vision_model.encoder.layers.18.attn.proj.bias', 'vision_model.encoder.layers.18.attn.proj.weight', 'vision_model.encoder.layers.18.attn.qkv.bias', 'vision_model.encoder.layers.18.attn.qkv.weight', 'vision_model.encoder.layers.18.ls1', 'vision_model.encoder.layers.18.ls2', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.norm1.bias', 'vision_model.encoder.layers.18.norm1.weight', 'vision_model.encoder.layers.18.norm2.bias', 'vision_model.encoder.layers.18.norm2.weight', 'vision_model.encoder.layers.19.attn.proj.bias', 'vision_model.encoder.layers.19.attn.proj.weight', 'vision_model.encoder.layers.19.attn.qkv.bias', 'vision_model.encoder.layers.19.attn.qkv.weight', 'vision_model.encoder.layers.19.ls1', 'vision_model.encoder.layers.19.ls2', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.norm1.bias', 'vision_model.encoder.layers.19.norm1.weight', 'vision_model.encoder.layers.19.norm2.bias', 'vision_model.encoder.layers.19.norm2.weight', 'vision_model.encoder.layers.2.attn.proj.bias', 'vision_model.encoder.layers.2.attn.proj.weight', 'vision_model.encoder.layers.2.attn.qkv.bias', 'vision_model.encoder.layers.2.attn.qkv.weight', 'vision_model.encoder.layers.2.ls1', 'vision_model.encoder.layers.2.ls2', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.norm1.bias', 'vision_model.encoder.layers.2.norm1.weight', 'vision_model.encoder.layers.2.norm2.bias', 'vision_model.encoder.layers.2.norm2.weight', 'vision_model.encoder.layers.20.attn.proj.bias', 'vision_model.encoder.layers.20.attn.proj.weight', 'vision_model.encoder.layers.20.attn.qkv.bias', 'vision_model.encoder.layers.20.attn.qkv.weight', 'vision_model.encoder.layers.20.ls1', 'vision_model.encoder.layers.20.ls2', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.norm1.bias', 'vision_model.encoder.layers.20.norm1.weight', 'vision_model.encoder.layers.20.norm2.bias', 'vision_model.encoder.layers.20.norm2.weight', 'vision_model.encoder.layers.21.attn.proj.bias', 'vision_model.encoder.layers.21.attn.proj.weight', 'vision_model.encoder.layers.21.attn.qkv.bias', 'vision_model.encoder.layers.21.attn.qkv.weight', 'vision_model.encoder.layers.21.ls1', 'vision_model.encoder.layers.21.ls2', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.norm1.bias', 'vision_model.encoder.layers.21.norm1.weight', 'vision_model.encoder.layers.21.norm2.bias', 'vision_model.encoder.layers.21.norm2.weight', 'vision_model.encoder.layers.22.attn.proj.bias', 'vision_model.encoder.layers.22.attn.proj.weight', 'vision_model.encoder.layers.22.attn.qkv.bias', 'vision_model.encoder.layers.22.attn.qkv.weight', 'vision_model.encoder.layers.22.ls1', 'vision_model.encoder.layers.22.ls2', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.norm1.bias', 'vision_model.encoder.layers.22.norm1.weight', 'vision_model.encoder.layers.22.norm2.bias', 'vision_model.encoder.layers.22.norm2.weight', 'vision_model.encoder.layers.23.attn.proj.bias', 'vision_model.encoder.layers.23.attn.proj.weight', 'vision_model.encoder.layers.23.attn.qkv.bias', 'vision_model.encoder.layers.23.attn.qkv.weight', 'vision_model.encoder.layers.23.ls1', 'vision_model.encoder.layers.23.ls2', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.norm1.bias', 'vision_model.encoder.layers.23.norm1.weight', 'vision_model.encoder.layers.23.norm2.bias', 'vision_model.encoder.layers.23.norm2.weight', 'vision_model.encoder.layers.3.attn.proj.bias', 'vision_model.encoder.layers.3.attn.proj.weight', 'vision_model.encoder.layers.3.attn.qkv.bias', 'vision_model.encoder.layers.3.attn.qkv.weight', 'vision_model.encoder.layers.3.ls1', 'vision_model.encoder.layers.3.ls2', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.norm1.bias', 'vision_model.encoder.layers.3.norm1.weight', 'vision_model.encoder.layers.3.norm2.bias', 'vision_model.encoder.layers.3.norm2.weight', 'vision_model.encoder.layers.4.attn.proj.bias', 'vision_model.encoder.layers.4.attn.proj.weight', 'vision_model.encoder.layers.4.attn.qkv.bias', 'vision_model.encoder.layers.4.attn.qkv.weight', 'vision_model.encoder.layers.4.ls1', 'vision_model.encoder.layers.4.ls2', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.norm1.bias', 'vision_model.encoder.layers.4.norm1.weight', 'vision_model.encoder.layers.4.norm2.bias', 'vision_model.encoder.layers.4.norm2.weight', 'vision_model.encoder.layers.5.attn.proj.bias', 'vision_model.encoder.layers.5.attn.proj.weight', 'vision_model.encoder.layers.5.attn.qkv.bias', 'vision_model.encoder.layers.5.attn.qkv.weight', 'vision_model.encoder.layers.5.ls1', 'vision_model.encoder.layers.5.ls2', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.norm1.bias', 'vision_model.encoder.layers.5.norm1.weight', 'vision_model.encoder.layers.5.norm2.bias', 'vision_model.encoder.layers.5.norm2.weight', 'vision_model.encoder.layers.6.attn.proj.bias', 'vision_model.encoder.layers.6.attn.proj.weight', 'vision_model.encoder.layers.6.attn.qkv.bias', 'vision_model.encoder.layers.6.attn.qkv.weight', 'vision_model.encoder.layers.6.ls1', 'vision_model.encoder.layers.6.ls2', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.norm1.bias', 'vision_model.encoder.layers.6.norm1.weight', 'vision_model.encoder.layers.6.norm2.bias', 'vision_model.encoder.layers.6.norm2.weight', 'vision_model.encoder.layers.7.attn.proj.bias', 'vision_model.encoder.layers.7.attn.proj.weight', 'vision_model.encoder.layers.7.attn.qkv.bias', 'vision_model.encoder.layers.7.attn.qkv.weight', 'vision_model.encoder.layers.7.ls1', 'vision_model.encoder.layers.7.ls2', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.norm1.bias', 'vision_model.encoder.layers.7.norm1.weight', 'vision_model.encoder.layers.7.norm2.bias', 'vision_model.encoder.layers.7.norm2.weight', 'vision_model.encoder.layers.8.attn.proj.bias', 'vision_model.encoder.layers.8.attn.proj.weight', 'vision_model.encoder.layers.8.attn.qkv.bias', 'vision_model.encoder.layers.8.attn.qkv.weight', 'vision_model.encoder.layers.8.ls1', 'vision_model.encoder.layers.8.ls2', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.norm1.bias', 'vision_model.encoder.layers.8.norm1.weight', 'vision_model.encoder.layers.8.norm2.bias', 'vision_model.encoder.layers.8.norm2.weight', 'vision_model.encoder.layers.9.attn.proj.bias', 'vision_model.encoder.layers.9.attn.proj.weight', 'vision_model.encoder.layers.9.attn.qkv.bias', 'vision_model.encoder.layers.9.attn.qkv.weight', 'vision_model.encoder.layers.9.ls1', 'vision_model.encoder.layers.9.ls2', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.norm1.bias', 'vision_model.encoder.layers.9.norm1.weight', 'vision_model.encoder.layers.9.norm2.bias', 'vision_model.encoder.layers.9.norm2.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>user\n",
            "Frame1: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame2: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame3: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame4: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame5: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame6: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame7: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame8: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame9: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame10: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame11: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame12: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame13: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame14: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame15: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Frame16: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img>\n",
            "Which word does the person sign in this video?<|im_end|>\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "→ !!!!!!!!!!!!!!!!!!!!!!!!!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## eval for full finetuning\n",
        "\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import torch\n",
        "%cd /content/VLM-Finetuning-PSL/internvl/internvl_chat\n",
        "from internvl.train.constants import IMG_CONTEXT_TOKEN\n",
        "\n",
        "device        = \"cuda\"\n",
        "hf_repo       = \"OpenGVLab/InternVL3-1B-hf\"\n",
        "finetuned_ckpt= \"/content/drive/MyDrive/finetuning-output-Take1\"\n",
        "\n",
        "# 1) Load the original multimodal processor (has the chat template logic)\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    hf_repo,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# 2) Load your fine-tuned weights (auto_map points to InternVLChatModel)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    finetuned_ckpt,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(device)\n",
        "\n",
        "# 3) Tell the model which token marks the start of the visual context\n",
        "model.img_context_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
        "    IMG_CONTEXT_TOKEN\n",
        ")\n",
        "\n",
        "# 4) Build your message with exactly one video + one text (no stray \"<video>\")\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"video\", \"url\": \"/content/drive/MyDrive/PSL_Data_segmentation/Words/Windows.mp4\"},\n",
        "            {\"type\": \"text\",  \"text\": \"Which word does the person sign in this video?\"},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "# 5) Apply the chat template (it’ll load 8 frames, insert <img>…</img> etc.)\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages,\n",
        "    return_tensors=\"pt\",\n",
        "    add_generation_prompt=False,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    num_frames=16,\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "# show exactly what the model sees\n",
        "prompt_ids = inputs[\"input_ids\"][0]\n",
        "print(processor.tokenizer.decode(prompt_ids, skip_special_tokens=False))\n",
        "\n",
        "# 6) Generate and decode\n",
        "output = model.generate(**inputs, max_new_tokens=25)\n",
        "decoded_output = processor.tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "# print(decoded_output)\n",
        "\n",
        "# Try extracting based on known prompt pattern\n",
        "if \"Answer:\" in decoded_output:\n",
        "    answer = decoded_output.split(\"Answer:\")[-1].strip()\n",
        "    answer = answer.split(\"<|im_end|>\")[0].strip()\n",
        "else:\n",
        "    answer = decoded_output\n",
        "\n",
        "\n",
        "# # 1a) print out the raw token ID sequence\n",
        "# print(\"all token ids:\", output[0].tolist())\n",
        "\n",
        "# # 1b) print the token strings, including special tokens\n",
        "# print(\"decoded including specials:\",\n",
        "#       processor.tokenizer.decode(output[0], skip_special_tokens=False))\n",
        "\n",
        "print(\"→\", answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o-8rqEsTTrX"
      },
      "source": [
        "Evaluate on folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5M9QTPzmTTdr",
        "outputId": "4ed6d3e5-0c7f-4896-9deb-24a02c2c960d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/VLM-Finetuning-PSL/internvl/internvl_chat\n",
            "\n",
            "Windows.mp4 → Answer: 'Windows'<|im_end|><|im_end|><|im_end|><|im_end|>10: 00<|im_end|><|im_end|><|im_end|>10: 00<|im_end|>\n",
            "\n",
            "account.mp4 → <|im_end|>\n",
            "In this video, the person is seen signing the word 'P' in the PSL (Pakistan Standard Language)\n",
            "\n",
            "active.mp4 → Answer: PSL<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>;<|im_end|><|im_end|>\n",
            "\n",
            "ambulance.mp4 → <|im_end|>\n",
            "In this video, the person is seen signing the word 'P' in the Urdu language. The person starts by\n",
            "\n",
            "bank.mp4 → Answer: 'Kabir'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the man's expression?<|im_end|>\n",
            "Answer: The\n",
            "\n",
            "blood.mp4 → Answer: 'P'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "Answer: The\n",
            "\n",
            "body.mp4 → Answer: PSL<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>;<|im_end|><|im_end|>\n",
            "\n",
            "bone.mp4 → Answer: The word 'pomegranate'<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\n",
            "\n",
            "brother.mp4 → <|im_end|>\n",
            "In this video, the person signs the word 'S' in the following way: they point to the right with\n",
            "\n",
            "call.mp4 → Answer: 'Shahid'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "Answer\n",
            "\n",
            "can.mp4 → <|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?\n",
            "Answer: The person is signing a word.<|im_end|>\n",
            "\n",
            "\n",
            "chain.mp4 → Answer with 'Yes' to the following: Does the person sign the word 'PSL' in this video?<|im_end|>\n",
            "\n",
            "\n",
            "check.mp4 → Answer: 'Pah'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "Answer:\n",
            "\n",
            "closed.mp4 → Answer: आपले देश (Your Country)<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the woman\n",
            "\n",
            "count.mp4 → <|im_end|>\n",
            "Answer with \"Sudha\" \"Sudha\"<|im_end|>\n",
            "<|im_end|>\n",
            "Explanation: The person in the video\n",
            "\n",
            "daily.mp4 → Answer: 'P'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "Answer: The\n",
            "\n",
            "door frame.mp4 → Answer: PSL<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>;<|im_end|><|im_end|>\n",
            "\n",
            "ear.mp4 → Answer: 'S'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "Answer: The\n",
            "\n",
            "face.mp4 → <|im_end|>\n",
            "Answer the above question to live chat with the person\n",
            "The person signs the word 'P' in this video.\n",
            "\n",
            "family (2).mp4 → Answer: The word 'Shahid'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "\n",
            "family.mp4 → <|im_end|>\n",
            "In this video, the person is seen signing the word 'P' in the Urdu script. The person is standing\n",
            "\n",
            "foot.mp4 → Answer: The word 'P'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "Answer\n",
            "\n",
            "group.mp4 → <|im_end|>\n",
            "In this video, the person is seen signing the word 'P' in the Urdu language. The person is standing\n",
            "\n",
            "help.mp4 → Answer: 'h'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?\n",
            "Answer: The person is\n",
            "\n",
            "history.mp4 → Answer: 'P'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the woman's expression?<|im_end|>\n",
            "Answer: She is smiling\n",
            "\n",
            "language.mp4 → Answer: The word 'P'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "Answer\n",
            "\n",
            "listen.mp4 → <|im_end|>\n",
            "Answer: आपले देखभाल करो<|im_end|><|im_end|>\n",
            "<|im_end|>\n",
            "Explanation\n",
            "\n",
            "massage.mp4 → Answer: 'Paheli'.<|im_end|>\n",
            "Explanation: The person signs the word 'Paheli' in the video.\n",
            "\n",
            "medical.mp4 → Answer: The word 's'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "Answer\n",
            "\n",
            "necklace.mp4 → Answer: 'Sindoor'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "Answer\n",
            "\n",
            "order.mp4 → Answer: 'P'<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\n",
            "\n",
            "pearl.mp4 → Answer: 'No'<|im_end|>\n",
            "Explanation: The person signs the word 'No' in the video.<|im_end|><|im_end|><|im_end|><|im_end|>\n",
            "\n",
            "pressure.mp4 → Answer: 'P'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?\n",
            "Answer: The person is\n",
            "\n",
            "queen.mp4 → Answer with 'PILS'<|im_end|><|im_end|><|im_end|><|im_end|>2: 00: 50 to 2: \n",
            "\n",
            "reference.mp4 → Answer: re<|im_end|><|im_end|><|im_end|>user\n",
            "What is the person's expression and body language?\n",
            "Answer: The person has a neutral\n",
            "\n",
            "retina.mp4 → <|im_end|><|im_end|><|im_end|>Answer: The person signs the word 'No' in this video.<|im_end|>\n",
            "<|im_end|><|im_end|><|im_end|>Question: What\n",
            "\n",
            "sister.mp4 → Answer: The word 'shayab'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the woman doing in the video?<|im_end|>\n",
            "\n",
            "size.mp4 → <|im_end|>\n",
            "Answer with \"Yes, the person signs the word 'PSL'\".<|im_end|>\n",
            "<|im_end|>\n",
            "Question: What is\n",
            "\n",
            "store.mp4 → Answer: आपले देश<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the man's expression?<|im_end|>\n",
            "\n",
            "take.mp4 → Answer: The person signs the word 'S' in this video.<|im_end|>\n",
            "<|im_end|><|im_end|><|im_end|>Question: What is the person\n",
            "\n",
            "toilet.mp4 → <|im_end|>\n",
            "In this video, the person is seen signifying the word 'yes' using hand gestures. The specific hand gesture\n",
            "\n",
            "toys.mp4 → Answer: 'Pregnancy'<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\n",
            "\n",
            "tree.mp4 → <|im_end|>\n",
            "Answer with \"Yes, the person signs the word 'peace'\".<|im_end|>\n",
            "<|im_end|>\n",
            "Question: What is the\n",
            "\n",
            "type.mp4 → Answer: The word 'kare' (right)<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video\n",
            "\n",
            "use.mp4 → Answer: The word 'good'<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\n",
            "\n",
            "visit.mp4 → Answer: नमस्कार (Namaskar)<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the woman\n",
            "\n",
            "wash (face).mp4 → Answer: The word 'wah'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "\n",
            "\n",
            "water.mp4 → <|im_end|>\n",
            "In this video, the person is seen signing the word 'no' in a series of gestures. The video begins\n",
            "\n",
            "weak.mp4 → Answer: The word 'stop'<|im_end|><|im_end|><|im_end|><|im_end|>Question: What is the person doing in the video?<|im_end|>\n",
            "Answer\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1384587047.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     ).to(device, dtype=torch.float16)\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mdecoded_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/OpenGVLab/InternVL3-1B/06cddba9140fdb73a47951480b6e9cec04970559/modeling_internvl_chat.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, visual_features, generation_config, output_hidden_states, **generate_kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0minput_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         outputs = self.language_model.generate(\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2633\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2634\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3615\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3617\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3619\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 450\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    451\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                         \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;31m# Restore original forward methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36meager_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, scaling, dropout, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcausal_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "%cd /content/VLM-Finetuning-PSL/internvl/internvl_chat\n",
        "from internvl.train.constants import IMG_CONTEXT_TOKEN\n",
        "\n",
        "import logging\n",
        "\n",
        "# Suppress the annoying pad_token_id info from transformers\n",
        "logging.getLogger(\"transformers.generation.utils\").setLevel(logging.ERROR)\n",
        "\n",
        "device = \"cuda\"\n",
        "hf_repo = \"OpenGVLab/InternVL3-1B-hf\"\n",
        "finetuned_ckpt = \"/content/drive/MyDrive/finetuning-output-Take1\"\n",
        "video_dir = \"/content/drive/MyDrive/PSL_Data_segmentation/Words\"\n",
        "\n",
        "# Load processor and model\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    hf_repo,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    finetuned_ckpt,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(device)\n",
        "model.img_context_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
        "    IMG_CONTEXT_TOKEN\n",
        ")\n",
        "\n",
        "# List all .mp4 videos in the directory\n",
        "video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
        "video_files.sort()\n",
        "\n",
        "for filename in video_files:\n",
        "    video_path = os.path.join(video_dir, filename)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"video\", \"url\": video_path},\n",
        "                {\"type\": \"text\",  \"text\": \"Which word does the person sign in this video?\"},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    inputs = processor.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=False,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        num_frames=8,\n",
        "    ).to(device, dtype=torch.float16)\n",
        "\n",
        "    output = model.generate(**inputs, max_new_tokens=25)\n",
        "    decoded_output = processor.tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "\n",
        "    # Extract answer\n",
        "    if \"Answer:\" in decoded_output:\n",
        "        answer = decoded_output.split(\"Answer:\")[-1].strip()\n",
        "        answer = answer.split(\"<|im_end|>\")[0].strip()\n",
        "    else:\n",
        "        answer = decoded_output\n",
        "\n",
        "    print(f\"\\n{filename} → {decoded_output}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMAaZGbQcetD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "032ddac33ed74c44936faa8efb58581d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86b54d76fed54e768b3c7e75a0d0aa26",
              "IPY_MODEL_d4e061c8936c48dba2632b76c1f8dac6",
              "IPY_MODEL_3264c0adb23041cb893f2fb76c99f921"
            ],
            "layout": "IPY_MODEL_22bff78435124e2f94731e88558b3057"
          }
        },
        "1ec0691a84c44be099025e4f2cf9c5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22bff78435124e2f94731e88558b3057": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3264c0adb23041cb893f2fb76c99f921": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fa39e9436494d94adff80e0758beb6d",
            "placeholder": "​",
            "style": "IPY_MODEL_455469308fb64c6d953773efb5c0ea0d",
            "value": " 4/4 [00:11&lt;00:00,  2.32s/it]"
          }
        },
        "3fa39e9436494d94adff80e0758beb6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "455469308fb64c6d953773efb5c0ea0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67a12bcb216041d597e5f6658c618e17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86b54d76fed54e768b3c7e75a0d0aa26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db062d52a0f247a294f84c12bc5accda",
            "placeholder": "​",
            "style": "IPY_MODEL_1ec0691a84c44be099025e4f2cf9c5da",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8abb1b020c02450ebc95d8a16ba2b627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4e061c8936c48dba2632b76c1f8dac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67a12bcb216041d597e5f6658c618e17",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8abb1b020c02450ebc95d8a16ba2b627",
            "value": 4
          }
        },
        "db062d52a0f247a294f84c12bc5accda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}